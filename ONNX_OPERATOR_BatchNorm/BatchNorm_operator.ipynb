{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVlaTzeKrS3K",
        "outputId": "52d1703f-777d-43d3-c349-f0f4741ca5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root_v6.30.04_Ubunt 100%[===================>] 272.11M  37.3MB/s    in 7.3s    \n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "g++ set to manually installed.\n",
            "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
            "gcc set to manually installed.\n",
            "gfortran is already the newest version (4:11.2.0-1ubuntu1).\n",
            "libxext-dev is already the newest version (2:1.3.4-1build1).\n",
            "libxext-dev set to manually installed.\n",
            "libxft-dev is already the newest version (2.3.4-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "dpkg-dev is already the newest version (1.21.1ubuntu2.3).\n",
            "dpkg-dev set to manually installed.\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-dev set to manually installed.\n",
            "tar is already the newest version (1.34+dfsg-1ubuntu0.1.22.04.2).\n",
            "libpython3.11-dev is already the newest version (3.11.11-1+jammy1).\n",
            "libpython3.11-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  binutils-common binutils-x86-64-linux-gnu libapr1 libaprutil1 libbinutils libctf0 libserf-1-1\n",
            "  libsvn1 libutf8proc2\n",
            "Suggested packages:\n",
            "  binutils-doc db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 libutf8proc2 libxpm-dev subversion\n",
            "The following packages will be upgraded:\n",
            "  binutils binutils-common binutils-x86-64-linux-gnu libbinutils libctf0\n",
            "5 upgraded, 7 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 6,080 kB of archives.\n",
            "After this operation, 10.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.7 [103 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.7 [2,326 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.7 [662 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.7 [3,196 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.7 [222 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libapr1 amd64 1.7.0-8ubuntu0.22.04.2 [108 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libaprutil1 amd64 1.6.1-5ubuntu4.22.04.2 [92.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libserf-1-1 amd64 1.3.9-10ubuntu2 [50.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libutf8proc2 amd64 2.7.0-3 [73.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsvn1 amd64 1.14.1-3ubuntu0.22.04.1 [1,387 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxpm-dev amd64 1:3.5.12-1ubuntu0.22.04.2 [90.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 subversion amd64 1.14.1-3ubuntu0.22.04.1 [960 kB]\n",
            "Fetched 6,080 kB in 3s (2,242 kB/s)\n",
            "(Reading database ... 125044 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libctf0_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking libctf0:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../01-binutils-x86-64-linux-gnu_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../02-libbinutils_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../03-binutils_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../04-binutils-common_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "Preparing to unpack .../05-libapr1_1.7.0-8ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.7.0-8ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../06-libaprutil1_1.6.1-5ubuntu4.22.04.2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-5ubuntu4.22.04.2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../07-libserf-1-1_1.3.9-10ubuntu2_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-10ubuntu2) ...\n",
            "Selecting previously unselected package libutf8proc2:amd64.\n",
            "Preparing to unpack .../08-libutf8proc2_2.7.0-3_amd64.deb ...\n",
            "Unpacking libutf8proc2:amd64 (2.7.0-3) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../09-libsvn1_1.14.1-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libxpm-dev:amd64.\n",
            "Preparing to unpack .../10-libxpm-dev_1%3a3.5.12-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libxpm-dev:amd64 (1:3.5.12-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../11-subversion_1.14.1-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking subversion (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up libutf8proc2:amd64 (2.7.0-3) ...\n",
            "Setting up binutils-common:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libapr1:amd64 (1.7.0-8ubuntu0.22.04.2) ...\n",
            "Setting up libxpm-dev:amd64 (1:3.5.12-1ubuntu0.22.04.2) ...\n",
            "Setting up libbinutils:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-5ubuntu4.22.04.2) ...\n",
            "Setting up libctf0:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-10ubuntu2) ...\n",
            "Setting up libsvn1:amd64 (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.7) ...\n",
            "Setting up subversion (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up binutils (2.38-4ubuntu2.7) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2025-03-18 10:07:28--  http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 185.125.190.81, 91.189.91.81, 185.125.190.83, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|185.125.190.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318204 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M  1.09MB/s    in 1.2s    \n",
            "\n",
            "2025-03-18 10:07:30 (1.09 MB/s) - ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’ saved [1318204/1318204]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 125194 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download the pre-built ROOT tarball from GitHub Releases\n",
        "!wget -q --show-progress https://github.com/MohamedElashri/ROOT/releases/download/ubuntu/root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "# Step 2: Extract the ROOT files\n",
        "!unzip -q root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "\n",
        "# Step 3: Install missing system dependencies for ROOT\n",
        "!sudo ldconfig & apt-get install -y git dpkg-dev cmake g++ gcc binutils libx11-dev libxpm-dev libxft-dev libxext-dev tar gfortran subversion libpython3.11-dev\n",
        "\n",
        "# Step 4: Remove the tarball to free up space\n",
        "!rm -f root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "\n",
        "# Step 5: Install Compatible libssl\n",
        "\n",
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!rm -f libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/tmva_cuda_project/include/TMVA\n",
        "!mkdir -p /content/tmva_cuda_project/src\n",
        "!mkdir -p /content/tmva_cuda_project/test\n",
        "!mkdir -p /content/tmva_cuda_project/build"
      ],
      "metadata": {
        "id": "6xAWr0qoskKW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/ROperator_BatchNorm_CUDA.hxx\n",
        "#ifndef TMVA_SOFIE_ROPERATOR_BATCHNORM_CUDA\n",
        "#define TMVA_SOFIE_ROPERATOR_BATCHNORM_CUDA\n",
        "\n",
        "#include \"TMVA/ROperator.hxx\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "class ROperator_BatchNorm_CUDA final : public ROperator\n",
        "{\n",
        "private:\n",
        "   std::string fNX;      // Input tensor name\n",
        "   std::string fNY;      // Output tensor name\n",
        "   std::string fNScale;  // Scale tensor name\n",
        "   std::string fNBias;   // Bias tensor name\n",
        "   std::string fNMean;   // Mean tensor name\n",
        "   std::string fNVar;    // Variance tensor name\n",
        "\n",
        "   float fEpsilon;       // Epsilon for numerical stability\n",
        "   int fNumChannels;     // Number of channels (C dimension)\n",
        "   std::vector<size_t> fInputShape;   // Input tensor shape\n",
        "\n",
        "   int fSpatialDims;     // Number of spatial dimensions\n",
        "   std::string fType;    // Type string\n",
        "\n",
        "public:\n",
        "   ROperator_BatchNorm_CUDA() = default;\n",
        "\n",
        "   ROperator_BatchNorm_CUDA(std::string nameX, std::string nameScale, std::string nameBias,\n",
        "                            std::string nameMean, std::string nameVar, std::string nameY,\n",
        "                            float epsilon = 1e-5f);\n",
        "\n",
        "   // Required ROperator interface methods\n",
        "   std::vector<ETensorType> TypeInference(std::vector<ETensorType> input) {\n",
        "      return input;\n",
        "   }\n",
        "\n",
        "   std::vector<std::vector<size_t>> ShapeInference(std::vector<std::vector<size_t>> input) {\n",
        "      return {input[0]};  // Output shape matches input shape\n",
        "   }\n",
        "\n",
        "   void Initialize(RModel& model) override;\n",
        "   std::string Generate(std::string OpName) override;\n",
        "};\n",
        "\n",
        "// Declare specializations\n",
        "extern template class ROperator_BatchNorm_CUDA<float>;\n",
        "extern template class ROperator_BatchNorm_CUDA<double>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_ROPERATOR_BATCHNORM_CUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZXTOBdCs_YD",
        "outputId": "a2c0ea2f-52d2-4f7d-f410-93ae72401027"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/ROperator_BatchNorm_CUDA.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu\n",
        "#include \"TMVA/ROperator_BatchNorm_CUDA.hxx\"\n",
        "#include \"TMVA/RModel.hxx\"\n",
        "#include <sstream>\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "\n",
        "// CUDA kernel for BatchNorm (float version)\n",
        "__global__ void batchNormKernelFloat(const float* input, const float* scale, const float* bias,\n",
        "                                   const float* mean, const float* var, float* output,\n",
        "                                   size_t N, size_t C, size_t spatialSize, float epsilon) {\n",
        "    // Calculate global thread index\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Total elements = N * C * spatialSize\n",
        "    int total = N * C * spatialSize;\n",
        "\n",
        "    if (idx < total) {\n",
        "        // Calculate indices for the current element\n",
        "        int n = idx / (C * spatialSize);           // Batch index\n",
        "        int c = (idx / spatialSize) % C;           // Channel index\n",
        "        int spatialIdx = idx % spatialSize;        // Spatial index\n",
        "\n",
        "        // Apply batch normalization formula\n",
        "        float normalized = (input[idx] - mean[c]) / sqrtf(var[c] + epsilon);\n",
        "        output[idx] = scale[c] * normalized + bias[c];\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for BatchNorm (double version)\n",
        "__global__ void batchNormKernelDouble(const double* input, const double* scale, const double* bias,\n",
        "                                    const double* mean, const double* var, double* output,\n",
        "                                    size_t N, size_t C, size_t spatialSize, double epsilon) {\n",
        "    // Calculate global thread index\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Total elements = N * C * spatialSize\n",
        "    int total = N * C * spatialSize;\n",
        "\n",
        "    if (idx < total) {\n",
        "        // Calculate indices for the current element\n",
        "        int n = idx / (C * spatialSize);           // Batch index\n",
        "        int c = (idx / spatialSize) % C;           // Channel index\n",
        "        int spatialIdx = idx % spatialSize;        // Spatial index\n",
        "\n",
        "        // Apply batch normalization formula\n",
        "        double normalized = (input[idx] - mean[c]) / sqrt(var[c] + epsilon);\n",
        "        output[idx] = scale[c] * normalized + bias[c];\n",
        "    }\n",
        "}\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "ROperator_BatchNorm_CUDA<T>::ROperator_BatchNorm_CUDA(\n",
        "    std::string nameX, std::string nameScale, std::string nameBias,\n",
        "    std::string nameMean, std::string nameVar, std::string nameY, float epsilon) :\n",
        "    fNX(UTILITY::Clean_name(nameX)),\n",
        "    fNScale(UTILITY::Clean_name(nameScale)),\n",
        "    fNBias(UTILITY::Clean_name(nameBias)),\n",
        "    fNMean(UTILITY::Clean_name(nameMean)),\n",
        "    fNVar(UTILITY::Clean_name(nameVar)),\n",
        "    fNY(UTILITY::Clean_name(nameY)),\n",
        "    fEpsilon(epsilon),\n",
        "    fNumChannels(0),\n",
        "    fSpatialDims(0)\n",
        "{\n",
        "    fInputTensorNames = {fNX, fNScale, fNBias, fNMean, fNVar};\n",
        "    fOutputTensorNames = {fNY};\n",
        "\n",
        "    if (std::is_same<T, float>::value) {\n",
        "        fType = \"float\";\n",
        "    } else if (std::is_same<T, double>::value) {\n",
        "        fType = \"double\";\n",
        "    } else {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Encountered unsupported type parsing a BatchNorm CUDA operator\");\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "void ROperator_BatchNorm_CUDA<T>::Initialize(RModel& model)\n",
        "{\n",
        "    // Check if all input tensors exist\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Input tensor \" + fNX + \" not found in model\");\n",
        "    }\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNScale)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Scale tensor \" + fNScale + \" not found in model\");\n",
        "    }\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNBias)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Bias tensor \" + fNBias + \" not found in model\");\n",
        "    }\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNMean)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Mean tensor \" + fNMean + \" not found in model\");\n",
        "    }\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNVar)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Variance tensor \" + fNVar + \" not found in model\");\n",
        "    }\n",
        "\n",
        "    // Get input shape and validate\n",
        "    fInputShape = model.GetTensorShape(fNX);\n",
        "\n",
        "    // BatchNorm requires at least 2D tensor (N, C, ...) where C is channels\n",
        "    if (fInputShape.size() < 2) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Input tensor must have at least 2 dimensions\");\n",
        "    }\n",
        "\n",
        "    // Number of channels is the second dimension\n",
        "    fNumChannels = fInputShape[1];\n",
        "\n",
        "    // Number of spatial dimensions is total dims minus batch and channel dims\n",
        "    fSpatialDims = fInputShape.size() - 2;\n",
        "\n",
        "    // Validate parameters shapes (scale, bias, mean, var should all be 1D with C elements)\n",
        "    std::vector<size_t> scaleShape = model.GetTensorShape(fNScale);\n",
        "    std::vector<size_t> biasShape = model.GetTensorShape(fNBias);\n",
        "    std::vector<size_t> meanShape = model.GetTensorShape(fNMean);\n",
        "    std::vector<size_t> varShape = model.GetTensorShape(fNVar);\n",
        "\n",
        "    if (scaleShape.size() != 1 || scaleShape[0] != fNumChannels) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Scale tensor must be 1D with C elements\");\n",
        "    }\n",
        "    if (biasShape.size() != 1 || biasShape[0] != fNumChannels) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Bias tensor must be 1D with C elements\");\n",
        "    }\n",
        "    if (meanShape.size() != 1 || meanShape[0] != fNumChannels) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Mean tensor must be 1D with C elements\");\n",
        "    }\n",
        "    if (varShape.size() != 1 || varShape[0] != fNumChannels) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE BatchNorm CUDA Op: Variance tensor must be 1D with C elements\");\n",
        "    }\n",
        "\n",
        "    // Add output tensor with same shape as input\n",
        "    model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fInputShape);\n",
        "\n",
        "    if (model.Verbose()) {\n",
        "        std::cout << \"BatchNorm CUDA: \" << fNX << \" -> \" << fNY;\n",
        "        std::cout << \" (epsilon=\" << fEpsilon << \", channels=\" << fNumChannels;\n",
        "        std::cout << \", spatial_dims=\" << fSpatialDims << \")\" << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "std::string ROperator_BatchNorm_CUDA<T>::Generate(std::string OpName)\n",
        "{\n",
        "    OpName = \"op_\" + OpName;\n",
        "    if (fInputShape.empty()) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Operator BatchNorm CUDA called to Generate without being initialized first\");\n",
        "    }\n",
        "\n",
        "    std::stringstream out;\n",
        "\n",
        "    // Calculate dimensions\n",
        "    size_t batchSize = fInputShape[0];\n",
        "    size_t spatialSize = 1;\n",
        "    for (size_t i = 2; i < fInputShape.size(); i++) {\n",
        "        spatialSize *= fInputShape[i];\n",
        "    }\n",
        "    size_t totalSize = batchSize * fNumChannels * spatialSize;\n",
        "\n",
        "    // Begin code generation\n",
        "    out << \"\\n//------ BatchNorm CUDA (epsilon=\" << std::scientific << fEpsilon << \")\\n\";\n",
        "\n",
        "    // Define the CUDA kernel\n",
        "    out << SP << \"// CUDA kernel for BatchNorm operation\\n\";\n",
        "    out << SP << \"__global__ void \" << OpName << \"_batchnorm_kernel(const \" << fType << \"* input, \"\n",
        "        << \"const \" << fType << \"* scale, const \" << fType << \"* bias, \"\n",
        "        << \"const \" << fType << \"* mean, const \" << fType << \"* var, \"\n",
        "        << fType << \"* output, size_t N, size_t C, size_t spatialSize, \" << fType << \" epsilon) {\\n\";\n",
        "\n",
        "    out << SP << SP << \"// Calculate global thread index\\n\";\n",
        "    out << SP << SP << \"int idx = blockIdx.x * blockDim.x + threadIdx.x;\\n\\n\";\n",
        "\n",
        "    out << SP << SP << \"// Total elements = N * C * spatialSize\\n\";\n",
        "    out << SP << SP << \"int total = N * C * spatialSize;\\n\\n\";\n",
        "\n",
        "    out << SP << SP << \"if (idx < total) {\\n\";\n",
        "    out << SP << SP << SP << \"// Calculate indices for the current element\\n\";\n",
        "    out << SP << SP << SP << \"int n = idx / (C * spatialSize);           // Batch index\\n\";\n",
        "    out << SP << SP << SP << \"int c = (idx / spatialSize) % C;           // Channel index\\n\";\n",
        "    out << SP << SP << SP << \"int spatialIdx = idx % spatialSize;        // Spatial index\\n\\n\";\n",
        "\n",
        "    out << SP << SP << SP << \"// Apply batch normalization formula\\n\";\n",
        "    if (std::is_same<T, float>::value) {\n",
        "        out << SP << SP << SP << fType << \" normalized = (input[idx] - mean[c]) / sqrtf(var[c] + epsilon);\\n\";\n",
        "    } else {\n",
        "        out << SP << SP << SP << fType << \" normalized = (input[idx] - mean[c]) / sqrt(var[c] + epsilon);\\n\";\n",
        "    }\n",
        "    out << SP << SP << SP << \"output[idx] = scale[c] * normalized + bias[c];\\n\";\n",
        "    out << SP << SP << \"}\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Set up CUDA execution\n",
        "    out << SP << \"// Set up execution parameters\\n\";\n",
        "    out << SP << \"size_t N = \" << batchSize << \";\\n\";\n",
        "    out << SP << \"size_t C = \" << fNumChannels << \";\\n\";\n",
        "    out << SP << \"size_t spatialSize = \" << spatialSize << \";\\n\";\n",
        "    out << SP << \"size_t totalSize = \" << totalSize << \";\\n\";\n",
        "    out << SP << fType << \" epsilon = \" << std::scientific << fEpsilon << \";\\n\\n\";\n",
        "\n",
        "    out << SP << \"// Set up CUDA execution configuration\\n\";\n",
        "    out << SP << \"int blockSize = 256;\\n\";\n",
        "    out << SP << \"int numBlocks = (totalSize + blockSize - 1) / blockSize;\\n\\n\";\n",
        "\n",
        "    // Allocate device memory\n",
        "    out << SP << \"// Allocate device memory\\n\";\n",
        "    out << SP << fType << \"* d_input = nullptr;\\n\";\n",
        "    out << SP << fType << \"* d_scale = nullptr;\\n\";\n",
        "    out << SP << fType << \"* d_bias = nullptr;\\n\";\n",
        "    out << SP << fType << \"* d_mean = nullptr;\\n\";\n",
        "    out << SP << fType << \"* d_var = nullptr;\\n\";\n",
        "    out << SP << fType << \"* d_output = nullptr;\\n\";\n",
        "    out << SP << \"cudaError_t cudaStatus;\\n\\n\";\n",
        "\n",
        "    // Input tensor\n",
        "    out << SP << \"// Allocate and copy input tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_input, totalSize * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for input: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(d_input, tensor_\" << fNX << \", totalSize * sizeof(\" << fType << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy failed for input: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Scale tensor\n",
        "    out << SP << \"// Allocate and copy scale tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_scale, C * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for scale: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(d_scale, tensor_\" << fNScale << \", C * sizeof(\" << fType << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy failed for scale: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Bias tensor\n",
        "    out << SP << \"// Allocate and copy bias tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_bias, C * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for bias: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(d_bias, tensor_\" << fNBias << \", C * sizeof(\" << fType << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy failed for bias: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Mean tensor\n",
        "    out << SP << \"// Allocate and copy mean tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_mean, C * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for mean: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(d_mean, tensor_\" << fNMean << \", C * sizeof(\" << fType << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy failed for mean: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Var tensor\n",
        "    out << SP << \"// Allocate and copy variance tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_var, C * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for variance: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(d_var, tensor_\" << fNVar << \", C * sizeof(\" << fType << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy failed for variance: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Output tensor\n",
        "    out << SP << \"// Allocate output tensor\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMalloc(&d_output, totalSize * sizeof(\" << fType << \"));\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMalloc failed for output: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Launch kernel\n",
        "    out << SP << \"// Launch BatchNorm kernel\\n\";\n",
        "    out << SP << OpName << \"_batchnorm_kernel<<<numBlocks, blockSize>>>(d_input, d_scale, d_bias, d_mean, d_var, d_output, N, C, spatialSize, epsilon);\\n\\n\";\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    out << SP << \"// Check for kernel launch errors\\n\";\n",
        "    out << SP << \"cudaStatus = cudaGetLastError();\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"CUDA kernel launch failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Synchronize\n",
        "    out << SP << \"// Wait for kernel completion\\n\";\n",
        "    out << SP << \"cudaStatus = cudaDeviceSynchronize();\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaDeviceSynchronize failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Copy result back to host\n",
        "    out << SP << \"// Copy result back to host\\n\";\n",
        "    out << SP << \"cudaStatus = cudaMemcpy(tensor_\" << fNY << \", d_output, totalSize * sizeof(\" << fType << \"), cudaMemcpyDeviceToHost);\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"cudaMemcpy to host failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << SP << SP << \"goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << SP << \"}\\n\\n\";\n",
        "\n",
        "    // Cleanup\n",
        "    out << SP << OpName << \"_cleanup:\\n\";\n",
        "    out << SP << \"// Clean up device memory\\n\";\n",
        "    out << SP << \"if (d_input) cudaFree(d_input);\\n\";\n",
        "    out << SP << \"if (d_scale) cudaFree(d_scale);\\n\";\n",
        "    out << SP << \"if (d_bias) cudaFree(d_bias);\\n\";\n",
        "    out << SP << \"if (d_mean) cudaFree(d_mean);\\n\";\n",
        "    out << SP << \"if (d_var) cudaFree(d_var);\\n\";\n",
        "    out << SP << \"if (d_output) cudaFree(d_output);\\n\\n\";\n",
        "\n",
        "    // CPU fallback\n",
        "    out << SP << \"// CPU fallback if CUDA execution failed\\n\";\n",
        "    out << SP << \"if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << SP << SP << \"std::cerr << \\\"Using CPU fallback for BatchNorm operation\\\" << std::endl;\\n\";\n",
        "    out << SP << SP << \"for (size_t n = 0; n < N; n++) {\\n\";\n",
        "    out << SP << SP << SP << \"for (size_t c = 0; c < C; c++) {\\n\";\n",
        "    out << SP << SP << SP << SP << \"for (size_t s = 0; s < spatialSize; s++) {\\n\";\n",
        "    out << SP << SP << SP << SP << SP << \"size_t idx = n * C * spatialSize + c * spatialSize + s;\\n\";\n",
        "    out << SP << SP << SP << SP << SP << fType << \" normalized = (tensor_\" << fNX << \"[idx] - tensor_\" << fNMean << \"[c]) / \";\n",
        "    if (std::is_same<T, float>::value) {\n",
        "        out << \"sqrtf(tensor_\" << fNVar << \"[c] + epsilon);\\n\";\n",
        "    } else {\n",
        "        out << \"sqrt(tensor_\" << fNVar << \"[c] + epsilon);\\n\";\n",
        "    }\n",
        "    out << SP << SP << SP << SP << SP << \"tensor_\" << fNY << \"[idx] = tensor_\" << fNScale << \"[c] * normalized + tensor_\" << fNBias << \"[c];\\n\";\n",
        "    out << SP << SP << SP << SP << \"}\\n\";\n",
        "    out << SP << SP << SP << \"}\\n\";\n",
        "    out << SP << SP << \"}\\n\";\n",
        "    out << SP << \"}\\n\";\n",
        "\n",
        "    return out.str();\n",
        "}\n",
        "\n",
        "// Explicit template instantiations\n",
        "template class ROperator_BatchNorm_CUDA<float>;\n",
        "template class ROperator_BatchNorm_CUDA<double>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-DB2a2tIAI",
        "outputId": "721e0afc-2343-41d1-ebe6-eab14e4473e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/test/test_batchnorm_cuda.cu\n",
        "#include \"TMVA/ROperator_BatchNorm_CUDA.hxx\"\n",
        "#include \"TMVA/RModel.hxx\"\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <iomanip>\n",
        "#include <cmath>\n",
        "#include <cassert>\n",
        "#include <algorithm>\n",
        "#include <numeric>\n",
        "\n",
        "using namespace TMVA::Experimental::SOFIE;\n",
        "\n",
        "// Helper function to compare floating point values with tolerance\n",
        "template <typename T>\n",
        "bool isClose(T a, T b, T rtol = 1e-5, T atol = 1e-8) {\n",
        "    return std::abs(a - b) <= atol + rtol * std::abs(b);\n",
        "}\n",
        "\n",
        "// Helper function to compare vectors with tolerance\n",
        "template <typename T>\n",
        "bool allClose(const std::vector<T>& a, const std::vector<T>& b, T rtol = 1e-5, T atol = 1e-8) {\n",
        "    if (a.size() != b.size()) return false;\n",
        "    for (size_t i = 0; i < a.size(); i++) {\n",
        "        if (!isClose(a[i], b[i], rtol, atol)) {\n",
        "            std::cout << \"Mismatch at index \" << i << \": \" << a[i] << \" vs \" << b[i] << std::endl;\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "// Function to print tensor data\n",
        "template <typename T>\n",
        "void printTensor(const std::vector<T>& data, const std::vector<size_t>& shape, const std::string& name) {\n",
        "    std::cout << name << \" tensor shape: [\";\n",
        "    for (size_t i = 0; i < shape.size(); i++) {\n",
        "        std::cout << shape[i];\n",
        "        if (i < shape.size() - 1) std::cout << \", \";\n",
        "    }\n",
        "    std::cout << \"]\" << std::endl;\n",
        "\n",
        "    size_t maxPrint = 10; // Limit print to save space\n",
        "    std::cout << \"Values (first \" << maxPrint << \" elements): \";\n",
        "    for (size_t i = 0; i < std::min(data.size(), maxPrint); i++) {\n",
        "        std::cout << std::fixed << std::setprecision(4) << data[i] << \" \";\n",
        "    }\n",
        "    if (data.size() > maxPrint) std::cout << \"...\";\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "// CPU implementation of BatchNorm for comparison\n",
        "template <typename T>\n",
        "std::vector<T> cpuBatchNorm(const std::vector<T>& input, const std::vector<T>& scale,\n",
        "                           const std::vector<T>& bias, const std::vector<T>& mean,\n",
        "                           const std::vector<T>& var, const std::vector<size_t>& inputShape,\n",
        "                           T epsilon) {\n",
        "    std::vector<T> output(input.size());\n",
        "\n",
        "    // Calculate dimensions\n",
        "    size_t batchSize = inputShape[0];\n",
        "    size_t channels = inputShape[1];\n",
        "    size_t spatialSize = 1;\n",
        "    for (size_t i = 2; i < inputShape.size(); i++) {\n",
        "        spatialSize *= inputShape[i];\n",
        "    }\n",
        "\n",
        "    // Apply BatchNorm formula\n",
        "    for (size_t n = 0; n < batchSize; n++) {\n",
        "        for (size_t c = 0; c < channels; c++) {\n",
        "            for (size_t s = 0; s < spatialSize; s++) {\n",
        "                size_t idx = n * channels * spatialSize + c * spatialSize + s;\n",
        "                T normalized = (input[idx] - mean[c]) / std::sqrt(var[c] + epsilon);\n",
        "                output[idx] = scale[c] * normalized + bias[c];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return output;\n",
        "}\n",
        "\n",
        "// Test cases for BatchNorm\n",
        "void testBasicBatchNorm() {\n",
        "    std::cout << \"\\n=== Basic BatchNorm Test ===\\n\" << std::endl;\n",
        "\n",
        "    // Create a model\n",
        "    RModel model(\"bn_basic_test\", \"2025-03-14\");\n",
        "\n",
        "    // Define input tensor dimensions (NCHW format)\n",
        "    std::vector<size_t> inputShape = {2, 3, 2, 2};  // Small tensor for easy verification\n",
        "    const size_t N = inputShape[0];\n",
        "    const size_t C = inputShape[1];\n",
        "    const size_t H = inputShape[2];\n",
        "    const size_t W = inputShape[3];\n",
        "    const size_t spatialSize = H * W;\n",
        "    const size_t totalSize = N * C * spatialSize;\n",
        "\n",
        "    // Create input tensor with specific pattern for verification\n",
        "    std::vector<float> inputData(totalSize);\n",
        "    for (size_t i = 0; i < totalSize; i++) {\n",
        "        inputData[i] = static_cast<float>(i) / 10.0f - 1.0f;  // Range: -1 to ~2.3\n",
        "    }\n",
        "\n",
        "    // Create parameters with specific values for verification\n",
        "    std::vector<float> scaleData = {1.0f, 2.0f, 0.5f};\n",
        "    std::vector<float> biasData = {0.0f, 0.1f, -0.1f};\n",
        "    std::vector<float> meanData = {0.5f, 0.0f, -0.5f};\n",
        "    std::vector<float> varData = {1.0f, 1.5f, 2.0f};\n",
        "\n",
        "    // Add tensors to model\n",
        "    model.AddInputTensorInfo(\"input\", ETensorType::FLOAT, inputShape);\n",
        "    model.AddInputTensorInfo(\"scale\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"bias\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"mean\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"var\", ETensorType::FLOAT, {C});\n",
        "\n",
        "    // Set epsilon and initialize the model\n",
        "    float epsilon = 1e-5f;\n",
        "    model.Initialize();\n",
        "\n",
        "    // Create BatchNorm CUDA operator\n",
        "    ROperator_BatchNorm_CUDA<float> batchNormOp(\"input\", \"scale\", \"bias\", \"mean\", \"var\", \"output\", epsilon);\n",
        "\n",
        "    // Initialize operator\n",
        "    batchNormOp.Initialize(model);\n",
        "\n",
        "    // Generate code\n",
        "    std::string generatedCode = batchNormOp.Generate(\"TestBatchNorm\");\n",
        "\n",
        "    // Verify generated code contains expected elements\n",
        "    std::vector<std::string> expectedStrings = {\n",
        "        \"BatchNorm CUDA\",\n",
        "        \"epsilon\",\n",
        "        \"batchnorm_kernel\",\n",
        "        \"cudaMalloc\",\n",
        "        \"cudaMemcpy\"\n",
        "    };\n",
        "\n",
        "    for (const auto& str : expectedStrings) {\n",
        "        if (generatedCode.find(str) == std::string::npos) {\n",
        "            std::cerr << \"Generated code missing expected string: \" << str << std::endl;\n",
        "            assert(false);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Calculate reference output using CPU implementation\n",
        "    auto expectedOutput = cpuBatchNorm(inputData, scaleData, biasData, meanData, varData, inputShape, epsilon);\n",
        "\n",
        "    // Verify specific output values (we can only check the CPU fallback logic)\n",
        "    std::cout << \"Verifying CPU fallback calculations...\" << std::endl;\n",
        "\n",
        "    // Check a few specific indices\n",
        "    for (size_t c = 0; c < C; c++) {\n",
        "        // First element of each channel in first batch\n",
        "        size_t idx = c * spatialSize;\n",
        "        float input_val = inputData[idx];\n",
        "        float normalized = (input_val - meanData[c]) / std::sqrt(varData[c] + epsilon);\n",
        "        float expected_val = scaleData[c] * normalized + biasData[c];\n",
        "\n",
        "        std::cout << \"Channel \" << c << \": \";\n",
        "        std::cout << \"Input=\" << input_val << \", \";\n",
        "        std::cout << \"Expected output=\" << expected_val << \", \";\n",
        "        std::cout << \"Calculated=\" << expectedOutput[idx] << std::endl;\n",
        "\n",
        "        // Verify calculation\n",
        "        assert(isClose(expected_val, expectedOutput[idx]));\n",
        "    }\n",
        "\n",
        "    std::cout << \"\\nBasic BatchNorm test passed!\" << std::endl;\n",
        "}\n",
        "\n",
        "// Test edge cases\n",
        "void testBatchNormEdgeCases() {\n",
        "    std::cout << \"\\n=== BatchNorm Edge Cases Test ===\\n\" << std::endl;\n",
        "\n",
        "    // Test with very small variance (near zero)\n",
        "    {\n",
        "        std::cout << \"Testing with small variance values...\" << std::endl;\n",
        "\n",
        "        RModel model(\"bn_small_var_test\", \"2025-03-14\");\n",
        "        std::vector<size_t> inputShape = {1, 2, 1, 1};  // Minimal shape\n",
        "        const size_t C = inputShape[1];\n",
        "\n",
        "        std::vector<float> inputData = {0.5f, -0.5f};\n",
        "        std::vector<float> scaleData = {1.0f, 1.0f};\n",
        "        std::vector<float> biasData = {0.0f, 0.0f};\n",
        "        std::vector<float> meanData = {0.0f, 0.0f};\n",
        "        std::vector<float> varData = {1e-8f, 1e-8f};  // Very small variance\n",
        "\n",
        "        model.AddInputTensorInfo(\"input\", ETensorType::FLOAT, inputShape);\n",
        "        model.AddInputTensorInfo(\"scale\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"bias\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"mean\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"var\", ETensorType::FLOAT, {C});\n",
        "\n",
        "        float epsilon = 1e-5f;  // Epsilon will prevent division by zero\n",
        "        model.Initialize();\n",
        "\n",
        "        ROperator_BatchNorm_CUDA<float> batchNormOp(\"input\", \"scale\", \"bias\", \"mean\", \"var\", \"output\", epsilon);\n",
        "        batchNormOp.Initialize(model);\n",
        "        std::string generatedCode = batchNormOp.Generate(\"TestSmallVar\");\n",
        "\n",
        "        // Verify the code includes epsilon in the correct places\n",
        "        if (generatedCode.find(\"var[c] + epsilon\") == std::string::npos) {\n",
        "            std::cerr << \"Generated code doesn't properly handle epsilon for small variance\" << std::endl;\n",
        "            assert(false);\n",
        "        }\n",
        "\n",
        "        auto expectedOutput = cpuBatchNorm(inputData, scaleData, biasData, meanData, varData, inputShape, epsilon);\n",
        "        std::cout << \"Small variance test passed!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Test with different dimensions (3D tensor)\n",
        "    {\n",
        "        std::cout << \"Testing with 3D tensor (NCH)...\" << std::endl;\n",
        "\n",
        "        RModel model(\"bn_3d_test\", \"2025-03-14\");\n",
        "        std::vector<size_t> inputShape = {2, 3, 4};  // 3D tensor\n",
        "        const size_t C = inputShape[1];\n",
        "        const size_t totalSize = 2 * 3 * 4;\n",
        "\n",
        "        std::vector<float> inputData(totalSize);\n",
        "        std::iota(inputData.begin(), inputData.end(), 0.0f);  // Fill with 0, 1, 2, ...\n",
        "\n",
        "        std::vector<float> scaleData = {1.0f, 2.0f, 0.5f};\n",
        "        std::vector<float> biasData = {0.0f, 0.1f, -0.1f};\n",
        "        std::vector<float> meanData = {0.5f, 0.0f, -0.5f};\n",
        "        std::vector<float> varData = {1.0f, 1.5f, 2.0f};\n",
        "\n",
        "        model.AddInputTensorInfo(\"input\", ETensorType::FLOAT, inputShape);\n",
        "        model.AddInputTensorInfo(\"scale\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"bias\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"mean\", ETensorType::FLOAT, {C});\n",
        "        model.AddInputTensorInfo(\"var\", ETensorType::FLOAT, {C});\n",
        "\n",
        "        float epsilon = 1e-5f;\n",
        "        model.Initialize();\n",
        "\n",
        "        ROperator_BatchNorm_CUDA<float> batchNormOp(\"input\", \"scale\", \"bias\", \"mean\", \"var\", \"output\", epsilon);\n",
        "        batchNormOp.Initialize(model);\n",
        "\n",
        "        // Verify the operator initializes successfully with different dimensions\n",
        "        auto expectedOutput = cpuBatchNorm(inputData, scaleData, biasData, meanData, varData, inputShape, epsilon);\n",
        "        std::cout << \"3D tensor test passed!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Test with double precision\n",
        "    {\n",
        "        std::cout << \"Testing with double precision...\" << std::endl;\n",
        "\n",
        "        RModel model(\"bn_double_test\", \"2025-03-14\");\n",
        "        std::vector<size_t> inputShape = {1, 2, 2, 2};\n",
        "        const size_t C = inputShape[1];\n",
        "        const size_t totalSize = 1 * 2 * 2 * 2;\n",
        "\n",
        "        std::vector<double> inputData(totalSize, 1.0);\n",
        "        std::vector<double> scaleData = {1.0, 2.0};\n",
        "        std::vector<double> biasData = {0.0, 0.1};\n",
        "        std::vector<double> meanData = {0.5, 0.0};\n",
        "        std::vector<double> varData = {1.0, 1.5};\n",
        "\n",
        "        model.AddInputTensorInfo(\"input\", ETensorType::DOUBLE, inputShape);\n",
        "        model.AddInputTensorInfo(\"scale\", ETensorType::DOUBLE, {C});\n",
        "        model.AddInputTensorInfo(\"bias\", ETensorType::DOUBLE, {C});\n",
        "        model.AddInputTensorInfo(\"mean\", ETensorType::DOUBLE, {C});\n",
        "        model.AddInputTensorInfo(\"var\", ETensorType::DOUBLE, {C});\n",
        "\n",
        "        double epsilon = 1e-9;\n",
        "        model.Initialize();\n",
        "\n",
        "        ROperator_BatchNorm_CUDA<double> batchNormOp(\"input\", \"scale\", \"bias\", \"mean\", \"var\", \"output\", epsilon);\n",
        "        batchNormOp.Initialize(model);\n",
        "\n",
        "        std::string generatedCode = batchNormOp.Generate(\"TestDouble\");\n",
        "\n",
        "        // Verify double precision is used in the CUDA kernel\n",
        "        if (generatedCode.find(\"double\") == std::string::npos ||\n",
        "            generatedCode.find(\"sqrt(\") == std::string::npos) {  // double uses sqrt() not sqrtf()\n",
        "            std::cerr << \"Generated code doesn't properly handle double precision\" << std::endl;\n",
        "            assert(false);\n",
        "        }\n",
        "\n",
        "        auto expectedOutput = cpuBatchNorm(inputData, scaleData, biasData, meanData, varData, inputShape, epsilon);\n",
        "        std::cout << \"Double precision test passed!\" << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Performance test\n",
        "void testBatchNormPerformance() {\n",
        "    std::cout << \"\\n=== BatchNorm Performance Test ===\\n\" << std::endl;\n",
        "\n",
        "    // Use a more realistic tensor size for performance testing\n",
        "    RModel model(\"bn_perf_test\", \"2025-03-14\");\n",
        "    std::vector<size_t> inputShape = {32, 64, 56, 56};  // Common CNN layer size\n",
        "    const size_t N = inputShape[0];\n",
        "    const size_t C = inputShape[1];\n",
        "    const size_t totalSize = N * C * inputShape[2] * inputShape[3];\n",
        "\n",
        "    std::cout << \"Testing with tensor size: \" << totalSize << \" elements (\"\n",
        "              << (totalSize * sizeof(float) / (1024.0 * 1024.0)) << \" MB)\" << std::endl;\n",
        "\n",
        "    // Generate code and analyze kernel configuration\n",
        "    float epsilon = 1e-5f;\n",
        "\n",
        "    ROperator_BatchNorm_CUDA<float> batchNormOp(\"input\", \"scale\", \"bias\", \"mean\", \"var\", \"output\", epsilon);\n",
        "    model.AddInputTensorInfo(\"input\", ETensorType::FLOAT, inputShape);\n",
        "    model.AddInputTensorInfo(\"scale\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"bias\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"mean\", ETensorType::FLOAT, {C});\n",
        "    model.AddInputTensorInfo(\"var\", ETensorType::FLOAT, {C});\n",
        "\n",
        "    model.Initialize();\n",
        "    batchNormOp.Initialize(model);\n",
        "\n",
        "    std::string generatedCode = batchNormOp.Generate(\"TestPerf\");\n",
        "\n",
        "    // Extract block size and grid size information from generated code\n",
        "    size_t blockSize = 256;  // Default in our implementation\n",
        "    size_t numBlocks = (totalSize + blockSize - 1) / blockSize;\n",
        "\n",
        "    std::cout << \"CUDA kernel configuration:\" << std::endl;\n",
        "    std::cout << \"  Block size: \" << blockSize << \" threads\" << std::endl;\n",
        "    std::cout << \"  Grid size: \" << numBlocks << \" blocks\" << std::endl;\n",
        "    std::cout << \"  Total threads: \" << blockSize * numBlocks << \" (vs \" << totalSize << \" elements)\" << std::endl;\n",
        "\n",
        "    // Estimate memory usage\n",
        "    size_t inputMemory = totalSize * sizeof(float);\n",
        "    size_t paramMemory = C * 4 * sizeof(float);  // scale, bias, mean, var\n",
        "    size_t outputMemory = totalSize * sizeof(float);\n",
        "    size_t totalMemory = inputMemory + paramMemory + outputMemory;\n",
        "\n",
        "    std::cout << \"Estimated GPU memory usage:\" << std::endl;\n",
        "    std::cout << \"  Input tensor: \" << (inputMemory / (1024.0 * 1024.0)) << \" MB\" << std::endl;\n",
        "    std::cout << \"  Parameters: \" << (paramMemory / (1024.0)) << \" KB\" << std::endl;\n",
        "    std::cout << \"  Output tensor: \" << (outputMemory / (1024.0 * 1024.0)) << \" MB\" << std::endl;\n",
        "    std::cout << \"  Total: \" << (totalMemory / (1024.0 * 1024.0)) << \" MB\" << std::endl;\n",
        "\n",
        "    std::cout << \"Performance test analysis complete!\" << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    std::cout << \"=== TMVA SOFIE CUDA BatchNorm Unit Tests ===\" << std::endl;\n",
        "\n",
        "    try {\n",
        "        // Run all test cases\n",
        "        testBasicBatchNorm();\n",
        "        testBatchNormEdgeCases();\n",
        "        testBatchNormPerformance();\n",
        "\n",
        "        std::cout << \"\\nAll BatchNorm CUDA tests passed successfully!\" << std::endl;\n",
        "        return 0;\n",
        "    } catch (const std::exception& e) {\n",
        "        std::cerr << \"Error: \" << e.what() << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SDaWFP2tS_J",
        "outputId": "84c590f0-5fb9-475e-b0fa-68ca50b0fb52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/test/test_batchnorm_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.10)\n",
        "project(TMVA_SOFIE_CUDA CUDA CXX)\n",
        "\n",
        "# Set C++ standard\n",
        "set(CMAKE_CXX_STANDARD 14)\n",
        "set(CMAKE_CUDA_STANDARD 14)\n",
        "set(CMAKE_CUDA_ARCHITECTURES 70)\n",
        "\n",
        "# Find CUDA\n",
        "find_package(CUDA REQUIRED)\n",
        "\n",
        "# Include directories\n",
        "include_directories(\n",
        "    ${CMAKE_CURRENT_SOURCE_DIR}/include\n",
        "    ${CUDA_INCLUDE_DIRS}\n",
        ")\n",
        "\n",
        "# Add ReLU CUDA operator\n",
        "cuda_add_executable(test_relu_cuda\n",
        "    test/test_relu_cuda.cu\n",
        "    src/ROperator_Relu_CUDA.cu\n",
        ")\n",
        "\n",
        "# Add ELU CUDA operator\n",
        "cuda_add_executable(test_elu_cuda\n",
        "    test/test_elu_cuda.cu\n",
        "    src/ROperator_Elu_CUDA.cu\n",
        ")\n",
        "\n",
        "# Add BatchNorm CUDA operator (bonus exercise)\n",
        "cuda_add_executable(test_batchnorm_cuda\n",
        "    test/test_batchnorm_cuda.cu\n",
        "    src/ROperator_BatchNorm_CUDA.cu\n",
        ")\n",
        "\n",
        "# Link against CUDA libraries\n",
        "target_link_libraries(test_relu_cuda ${CUDA_LIBRARIES})\n",
        "target_link_libraries(test_elu_cuda ${CUDA_LIBRARIES})\n",
        "target_link_libraries(test_batchnorm_cuda ${CUDA_LIBRARIES})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgAiJAgtrLp",
        "outputId": "f7556f66-41c1-460e-bcb3-182246d0574f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.10)\n",
        "project(TMVA_SOFIE_CUDA CUDA CXX)\n",
        "\n",
        "# Set C++ standard\n",
        "set(CMAKE_CXX_STANDARD 14)\n",
        "set(CMAKE_CUDA_STANDARD 14)\n",
        "set(CMAKE_CUDA_ARCHITECTURES 70)\n",
        "\n",
        "# Find CUDA\n",
        "find_package(CUDA REQUIRED)\n",
        "\n",
        "# Include directories\n",
        "include_directories(\n",
        "    ${CMAKE_CURRENT_SOURCE_DIR}/include\n",
        "    ${CUDA_INCLUDE_DIRS}\n",
        ")\n",
        "\n",
        "# Add BatchNorm CUDA operator (bonus exercise)\n",
        "cuda_add_executable(test_batchnorm_cuda\n",
        "    test/test_batchnorm_cuda.cu\n",
        "    src/ROperator_BatchNorm_CUDA.cu\n",
        ")\n",
        "\n",
        "# Link against CUDA libraries\n",
        "target_link_libraries(test_batchnorm_cuda ${CUDA_LIBRARIES})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA7zvdsJt3Vi",
        "outputId": "cc3f80b3-456a-4144-eca4-88979456e18c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tmva_cuda_project/CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/ROperator.hxx\n",
        "#ifndef TMVA_SOFIE_ROPERATOR\n",
        "#define TMVA_SOFIE_ROPERATOR\n",
        "\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Forward declaration\n",
        "class RModel;\n",
        "\n",
        "// Base class for all operators\n",
        "class ROperator {\n",
        "public:\n",
        "    virtual ~ROperator() = default;\n",
        "\n",
        "    // Core required methods\n",
        "    virtual std::vector<std::vector<size_t>> ShapeInference(std::vector<std::vector<size_t>> input) = 0;\n",
        "    virtual std::vector<ETensorType> TypeInference(std::vector<ETensorType> input) = 0;\n",
        "    virtual void Initialize(RModel& model) = 0;\n",
        "    virtual std::string Generate(std::string OpName) = 0;\n",
        "\n",
        "    // Optional session-related methods (can be empty in mock implementation)\n",
        "    virtual std::string GenerateInitCode() { return \"\"; }\n",
        "    virtual std::string GenerateDeclCode() { return \"\"; }\n",
        "    virtual std::string GenerateSessionMembersCode(std::string /*opName*/) { return \"\"; }\n",
        "    virtual std::string Header() { return \"\"; }\n",
        "    virtual std::vector<std::string> GetBlasRoutines() { return {}; }\n",
        "    virtual std::vector<std::string> GetStdLibs() { return {}; }\n",
        "\n",
        "    // Common members\n",
        "    std::vector<std::string> fInputTensorNames;\n",
        "    std::vector<std::string> fOutputTensorNames;\n",
        "\n",
        "protected:\n",
        "    const std::string SP = \"   \"; // Space for indentation\n",
        "    bool fUseSession = false;     // Flag for session usage\n",
        "    bool fIsOutputConstant = false; // Flag for constant output tensors\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_ROPERATOR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wzi8OI_uNmB",
        "outputId": "eea61db9-77b2-4a26-f681-4eed88f49d66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/ROperator.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/SOFIE_common.hxx\n",
        "#ifndef TMVA_SOFIE_COMMON\n",
        "#define TMVA_SOFIE_COMMON\n",
        "\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "#include <stdexcept>\n",
        "#include <iostream>\n",
        "#include <unordered_map>\n",
        "#include <functional>\n",
        "#include <algorithm>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Basic tensor type enum (matching official implementation)\n",
        "enum class ETensorType {\n",
        "    UNDEFINED = 0, FLOAT = 1, UNINT8 = 2, INT8 = 3, UINT16 = 4, INT16 = 5,\n",
        "    INT32 = 6, INT64 = 7, STRING = 8, BOOL = 9, FLOAT16 = 10, DOUBLE = 11,\n",
        "    UINT32 = 12, UINT64 = 13, COMPLEX64 = 14, COMPLEX28 = 15, BFLOAT16 = 16\n",
        "};\n",
        "\n",
        "// Dimension structure for dynamic shapes\n",
        "struct Dim {\n",
        "    bool isParam = false;\n",
        "    size_t dim = 0;\n",
        "    std::string param;\n",
        "\n",
        "    // Constructors\n",
        "    Dim() {}\n",
        "    Dim(const std::string& p, size_t d = 0) : isParam(true), dim(d), param(p) {}\n",
        "    Dim(size_t d) : dim(d) {}\n",
        "\n",
        "    std::string GetVal() const {\n",
        "        return (isParam) ? param : std::to_string(dim);\n",
        "    }\n",
        "};\n",
        "\n",
        "struct InputTensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<Dim> shape;\n",
        "};\n",
        "\n",
        "struct TensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<size_t> shape;\n",
        "};\n",
        "\n",
        "struct DynamicTensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<Dim> shape;\n",
        "};\n",
        "\n",
        "// Helper functions - add inline to prevent multiple definition errors\n",
        "inline size_t ConvertShapeToLength(const std::vector<size_t>& shape) {\n",
        "    size_t length = 1;\n",
        "    for (auto& dim : shape) {\n",
        "        length *= dim;\n",
        "    }\n",
        "    return length;\n",
        "}\n",
        "\n",
        "inline std::string ConvertShapeToString(std::vector<size_t> shape) {\n",
        "    std::string result = \"{\";\n",
        "    for (size_t i = 0; i < shape.size(); i++) {\n",
        "        result += std::to_string(shape[i]);\n",
        "        if (i < shape.size() - 1) result += \", \";\n",
        "    }\n",
        "    result += \"}\";\n",
        "    return result;\n",
        "}\n",
        "\n",
        "// Get string representation of type\n",
        "template<typename T>\n",
        "inline std::string GetTensorTypeName() {\n",
        "    if (std::is_same<T, float>::value) return \"float\";\n",
        "    if (std::is_same<T, double>::value) return \"double\";\n",
        "    if (std::is_same<T, int64_t>::value) return \"int64_t\";\n",
        "    if (std::is_same<T, int32_t>::value) return \"int32_t\";\n",
        "    if (std::is_same<T, bool>::value) return \"bool\";\n",
        "    return \"unknown\";\n",
        "}\n",
        "\n",
        "// Get ETensorType from C++ type\n",
        "template<typename T>\n",
        "ETensorType GetTemplatedType(T) {\n",
        "    if (std::is_same<T, float>::value) return ETensorType::FLOAT;\n",
        "    if (std::is_same<T, double>::value) return ETensorType::DOUBLE;\n",
        "    if (std::is_same<T, int64_t>::value) return ETensorType::INT64;\n",
        "    if (std::is_same<T, int32_t>::value) return ETensorType::INT32;\n",
        "    if (std::is_same<T, bool>::value) return ETensorType::BOOL;\n",
        "    throw std::runtime_error(\"Unsupported type in GetTemplatedType\");\n",
        "}\n",
        "\n",
        "// Simple initialized tensor class - simplified version of the official one\n",
        "class InitializedTensor {\n",
        "public:\n",
        "    InitializedTensor() = default;\n",
        "    InitializedTensor(ETensorType type, const std::vector<size_t>& shape,\n",
        "                     std::shared_ptr<void> data, bool constant = false)\n",
        "        : fConstant(constant), fType(type), fShape(shape), fData(data) {}\n",
        "\n",
        "    ETensorType const &type() const { return fType; }\n",
        "    std::vector<std::size_t> const &shape() const { return fShape; }\n",
        "    std::shared_ptr<void> const &sharedptr() const { return fData; }\n",
        "\n",
        "    // Additional flags to match official behavior\n",
        "    bool IsConstantTensor() const { return fConstant; }\n",
        "    bool IsWeightTensor() const { return !fConstant && !fIsNotWritable; }\n",
        "    void SetNotWritable() { fIsNotWritable = true; }\n",
        "\n",
        "    template <class T = void>\n",
        "    T const *data() const {\n",
        "        return static_cast<T const *>(fData.get());\n",
        "    }\n",
        "\n",
        "private:\n",
        "    bool fConstant = false;      // Flag for constant tensors\n",
        "    bool fIsNotWritable = false; // Flag for not writable tensors\n",
        "    ETensorType fType;\n",
        "    std::vector<size_t> fShape;\n",
        "    std::shared_ptr<void> fData;\n",
        "};\n",
        "\n",
        "// Utility namespace\n",
        "namespace UTILITY {\n",
        "    inline std::string Clean_name(const std::string& name) {\n",
        "        return name; // Simplified for testing\n",
        "    }\n",
        "\n",
        "    // Check if two shapes are equal\n",
        "    inline bool AreSameShape(const std::vector<size_t>& a, const std::vector<size_t>& b) {\n",
        "        if (a.size() != b.size()) return false;\n",
        "        for (size_t i = 0; i < a.size(); i++) {\n",
        "            if (a[i] != b[i]) return false;\n",
        "        }\n",
        "        return true;\n",
        "    }\n",
        "}\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_COMMON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwnOjncAu2_O",
        "outputId": "c1faa246-ae2e-4a6d-db60-644f390be35f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/SOFIE_common.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/RModel.hxx\n",
        "#ifndef TMVA_SOFIE_RMODEL\n",
        "#define TMVA_SOFIE_RMODEL\n",
        "\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <unordered_map>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "#include <map>\n",
        "#include <algorithm>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Forward declaration\n",
        "class ROperator;\n",
        "\n",
        "// Mock RModel class for our implementation\n",
        "class RModel {\n",
        "private:\n",
        "    std::string fName;\n",
        "    std::string fParsedDateTime;\n",
        "    bool fIsInitialized = false;\n",
        "    int fVerbose = 1;\n",
        "\n",
        "    // Tensor storage\n",
        "    std::unordered_map<std::string, ETensorType> fTensorTypes;\n",
        "    std::unordered_map<std::string, std::vector<size_t>> fTensorShapes;\n",
        "    std::vector<std::string> fOutputTensorNames;\n",
        "    std::vector<std::string> fInputTensorNames;\n",
        "\n",
        "public:\n",
        "    RModel() = default;\n",
        "    RModel(std::string name, std::string parsedtime) : fName(name), fParsedDateTime(parsedtime) {}\n",
        "\n",
        "    int Verbose() const { return fVerbose; }\n",
        "\n",
        "    const std::vector<size_t>& GetTensorShape(const std::string& name) {\n",
        "        auto it = fTensorShapes.find(name);\n",
        "        if (it != fTensorShapes.end()) {\n",
        "            return it->second;\n",
        "        }\n",
        "        throw std::runtime_error(\"Tensor not found: \" + name);\n",
        "    }\n",
        "\n",
        "    const ETensorType& GetTensorType(const std::string& name) {\n",
        "        auto it = fTensorTypes.find(name);\n",
        "        if (it != fTensorTypes.end()) {\n",
        "            return it->second;\n",
        "        }\n",
        "        throw std::runtime_error(\"Tensor type not found: \" + name);\n",
        "    }\n",
        "\n",
        "    bool CheckIfTensorAlreadyExist(const std::string& name) const {\n",
        "        return fTensorShapes.find(name) != fTensorShapes.end();\n",
        "    }\n",
        "\n",
        "    // Add input tensor info\n",
        "    void AddInputTensorInfo(const std::string& name, ETensorType type, const std::vector<size_t>& shape) {\n",
        "        fTensorTypes[name] = type;\n",
        "        fTensorShapes[name] = shape;\n",
        "\n",
        "        // Also add to input tensor names if not already there\n",
        "        if (std::find(fInputTensorNames.begin(), fInputTensorNames.end(), name) == fInputTensorNames.end()) {\n",
        "            fInputTensorNames.push_back(name);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add intermediate tensor\n",
        "    void AddIntermediateTensor(const std::string& name, ETensorType type, const std::vector<size_t>& shape) {\n",
        "        fTensorTypes[name] = type;\n",
        "        fTensorShapes[name] = shape;\n",
        "    }\n",
        "\n",
        "    // Add output tensor names\n",
        "    void AddOutputTensorNameList(const std::vector<std::string>& names) {\n",
        "        fOutputTensorNames = names;\n",
        "    }\n",
        "\n",
        "    // Initialize model (simplified for mock)\n",
        "    void Initialize(int batchSize = -1) {\n",
        "        fIsInitialized = true;\n",
        "\n",
        "        if (Verbose()) {\n",
        "            std::cout << \"Model initialized with batch size: \" <<\n",
        "                (batchSize == -1 ? \"default\" : std::to_string(batchSize)) << std::endl;\n",
        "        }\n",
        "    }\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_RMODEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7HICd02u48b",
        "outputId": "babe36cd-74ce-4184-e57b-726ab24435b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/RModel.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/tmva_cuda_project && cmake -B build && cmake --build build\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_9cwlluu6uj",
        "outputId": "f01c12ba-3770-4bbf-9413-e343350ae0af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning (dev) at CMakeLists.txt:10 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "-- Configuring done (4.2s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/tmva_cuda_project/build\n",
            "[ 33%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_batchnorm_cuda.dir/test/test_batchnorm_cuda_generated_test_batchnorm_cuda.cu.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_batchnorm_cuda.dir/src/test_batchnorm_cuda_generated_ROperator_BatchNorm_CUDA.cu.o\u001b[0m\n",
            "/content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu(19): warning #177-D: variable \"n\" was declared but never referenced\n",
            "          int n = idx / (C * spatialSize);\n",
            "              ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu(21): warning #177-D: variable \"spatialIdx\" was declared but never referenced\n",
            "          int spatialIdx = idx % spatialSize;\n",
            "              ^\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu(41): warning #177-D: variable \"n\" was declared but never referenced\n",
            "          int n = idx / (C * spatialSize);\n",
            "              ^\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_BatchNorm_CUDA.cu(43): warning #177-D: variable \"spatialIdx\" was declared but never referenced\n",
            "          int spatialIdx = idx % spatialSize;\n",
            "              ^\n",
            "\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable test_batchnorm_cuda\u001b[0m\n",
            "[100%] Built target test_batchnorm_cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/tmva_cuda_project/build && ./test_batchnorm_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spZktIByu9Od",
        "outputId": "80454df7-9be9-4b96-b6a5-195bc3150521"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TMVA SOFIE CUDA BatchNorm Unit Tests ===\n",
            "\n",
            "=== Basic BatchNorm Test ===\n",
            "\n",
            "Model initialized with batch size: default\n",
            "BatchNorm CUDA: input -> output (epsilon=1e-05, channels=3, spatial_dims=2)\n",
            "Verifying CPU fallback calculations...\n",
            "Channel 0: Input=-1, Expected output=-1.49999, Calculated=-1.49999\n",
            "Channel 1: Input=-0.6, Expected output=-0.879793, Calculated=-0.879793\n",
            "Channel 2: Input=-0.2, Expected output=0.00606576, Calculated=0.00606576\n",
            "\n",
            "Basic BatchNorm test passed!\n",
            "\n",
            "=== BatchNorm Edge Cases Test ===\n",
            "\n",
            "Testing with small variance values...\n",
            "Model initialized with batch size: default\n",
            "BatchNorm CUDA: input -> output (epsilon=1e-05, channels=2, spatial_dims=2)\n",
            "Small variance test passed!\n",
            "Testing with 3D tensor (NCH)...\n",
            "Model initialized with batch size: default\n",
            "BatchNorm CUDA: input -> output (epsilon=1e-05, channels=3, spatial_dims=1)\n",
            "3D tensor test passed!\n",
            "Testing with double precision...\n",
            "Model initialized with batch size: default\n",
            "BatchNorm CUDA: input -> output (epsilon=1e-09, channels=2, spatial_dims=2)\n",
            "Double precision test passed!\n",
            "\n",
            "=== BatchNorm Performance Test ===\n",
            "\n",
            "Testing with tensor size: 6422528 elements (24.5 MB)\n",
            "Model initialized with batch size: default\n",
            "BatchNorm CUDA: input -> output (epsilon=1e-05, channels=64, spatial_dims=2)\n",
            "CUDA kernel configuration:\n",
            "  Block size: 256 threads\n",
            "  Grid size: 25088 blocks\n",
            "  Total threads: 6422528 (vs 6422528 elements)\n",
            "Estimated GPU memory usage:\n",
            "  Input tensor: 24.5 MB\n",
            "  Parameters: 1 KB\n",
            "  Output tensor: 24.5 MB\n",
            "  Total: 49.001 MB\n",
            "Performance test analysis complete!\n",
            "\n",
            "All BatchNorm CUDA tests passed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CICyq-FsvEv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}