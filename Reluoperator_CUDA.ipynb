{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcyKxHH7EfafHGMfRuJTQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaPandeyCN/ONNX_Operator_Implementation/blob/main/Reluoperator_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4-rBXLVXFu4",
        "outputId": "05a64d72-14a0-41a0-cdc3-6587decca82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root_v6.30.04_Ubunt 100%[===================>] 272.11M  7.32MB/s    in 4.6s    \n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "g++ set to manually installed.\n",
            "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
            "gcc set to manually installed.\n",
            "gfortran is already the newest version (4:11.2.0-1ubuntu1).\n",
            "libxext-dev is already the newest version (2:1.3.4-1build1).\n",
            "libxext-dev set to manually installed.\n",
            "libxft-dev is already the newest version (2.3.4-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "dpkg-dev is already the newest version (1.21.1ubuntu2.3).\n",
            "dpkg-dev set to manually installed.\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-dev set to manually installed.\n",
            "tar is already the newest version (1.34+dfsg-1ubuntu0.1.22.04.2).\n",
            "libpython3.11-dev is already the newest version (3.11.11-1+jammy1).\n",
            "libpython3.11-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  binutils-common binutils-x86-64-linux-gnu libapr1 libaprutil1 libbinutils libctf0 libserf-1-1\n",
            "  libsvn1 libutf8proc2\n",
            "Suggested packages:\n",
            "  binutils-doc db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 libutf8proc2 libxpm-dev subversion\n",
            "The following packages will be upgraded:\n",
            "  binutils binutils-common binutils-x86-64-linux-gnu libbinutils libctf0\n",
            "5 upgraded, 7 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 6,080 kB of archives.\n",
            "After this operation, 10.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libctf0 amd64 2.38-4ubuntu2.7 [103 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.38-4ubuntu2.7 [2,326 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libbinutils amd64 2.38-4ubuntu2.7 [662 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils amd64 2.38-4ubuntu2.7 [3,196 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 binutils-common amd64 2.38-4ubuntu2.7 [222 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libapr1 amd64 1.7.0-8ubuntu0.22.04.2 [108 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libaprutil1 amd64 1.6.1-5ubuntu4.22.04.2 [92.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libserf-1-1 amd64 1.3.9-10ubuntu2 [50.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libutf8proc2 amd64 2.7.0-3 [73.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsvn1 amd64 1.14.1-3ubuntu0.22.04.1 [1,387 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxpm-dev amd64 1:3.5.12-1ubuntu0.22.04.2 [90.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 subversion amd64 1.14.1-3ubuntu0.22.04.1 [960 kB]\n",
            "Fetched 6,080 kB in 3s (2,250 kB/s)\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libctf0_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking libctf0:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../01-binutils-x86-64-linux-gnu_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../02-libbinutils_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../03-binutils_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Preparing to unpack .../04-binutils-common_2.38-4ubuntu2.7_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.38-4ubuntu2.7) over (2.38-4ubuntu2.6) ...\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "Preparing to unpack .../05-libapr1_1.7.0-8ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.7.0-8ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../06-libaprutil1_1.6.1-5ubuntu4.22.04.2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-5ubuntu4.22.04.2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../07-libserf-1-1_1.3.9-10ubuntu2_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-10ubuntu2) ...\n",
            "Selecting previously unselected package libutf8proc2:amd64.\n",
            "Preparing to unpack .../08-libutf8proc2_2.7.0-3_amd64.deb ...\n",
            "Unpacking libutf8proc2:amd64 (2.7.0-3) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../09-libsvn1_1.14.1-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libxpm-dev:amd64.\n",
            "Preparing to unpack .../10-libxpm-dev_1%3a3.5.12-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libxpm-dev:amd64 (1:3.5.12-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../11-subversion_1.14.1-3ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking subversion (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up libutf8proc2:amd64 (2.7.0-3) ...\n",
            "Setting up binutils-common:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libapr1:amd64 (1.7.0-8ubuntu0.22.04.2) ...\n",
            "Setting up libxpm-dev:amd64 (1:3.5.12-1ubuntu0.22.04.2) ...\n",
            "Setting up libbinutils:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-5ubuntu4.22.04.2) ...\n",
            "Setting up libctf0:amd64 (2.38-4ubuntu2.7) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-10ubuntu2) ...\n",
            "Setting up libsvn1:amd64 (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.38-4ubuntu2.7) ...\n",
            "Setting up subversion (1.14.1-3ubuntu0.22.04.1) ...\n",
            "Setting up binutils (2.38-4ubuntu2.7) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2025-03-14 15:55:39--  http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 91.189.91.81, 185.125.190.81, 91.189.91.83, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|91.189.91.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318204 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M   842KB/s    in 1.5s    \n",
            "\n",
            "2025-03-14 15:55:41 (842 KB/s) - ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’ saved [1318204/1318204]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 125097 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download the pre-built ROOT tarball from GitHub Releases\n",
        "!wget -q --show-progress https://github.com/MohamedElashri/ROOT/releases/download/ubuntu/root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "# Step 2: Extract the ROOT files\n",
        "!unzip -q root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "\n",
        "# Step 3: Install missing system dependencies for ROOT\n",
        "!sudo ldconfig & apt-get install -y git dpkg-dev cmake g++ gcc binutils libx11-dev libxpm-dev libxft-dev libxext-dev tar gfortran subversion libpython3.11-dev\n",
        "\n",
        "# Step 4: Remove the tarball to free up space\n",
        "!rm -f root_v6.30.04_Ubuntu_Python3.11.zip\n",
        "\n",
        "# Step 5: Install Compatible libssl\n",
        "\n",
        "!wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!rm -f libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import ctypes\n",
        "\n",
        "# Step 1: Append ROOT paths to Python\n",
        "sys.path.append(\"root_build/\")\n",
        "sys.path.append(\"root_build/bin/\")\n",
        "sys.path.append(\"root_build/include/\")\n",
        "sys.path.append(\"root_build/lib/\")\n",
        "\n",
        "# Step 2: Load the required shared libraries (.so files)\n",
        "ctypes.cdll.LoadLibrary(\"root_build/lib/libCore.so\")\n",
        "ctypes.cdll.LoadLibrary(\"root_build/lib/libThread.so\")\n",
        "ctypes.cdll.LoadLibrary(\"root_build/lib/libTreePlayer.so\")\n",
        "\n",
        "print(\"ROOT Libraries Loaded Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yba-H5POXRLQ",
        "outputId": "a45feb7f-cfa7-466a-f9b5-28a4cecc5f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT Libraries Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/tmva_cuda_project/{include/TMVA,src,test,build}"
      ],
      "metadata": {
        "id": "DmGb0k1dXqFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/RTensor.hxx\n",
        "#ifndef TMVA_SOFIE_RTENSOR\n",
        "#define TMVA_SOFIE_RTENSOR\n",
        "\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "#include <stdexcept>\n",
        "#include <algorithm>\n",
        "#include <numeric>\n",
        "#include <functional>\n",
        "#include <iostream>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Forward declaration\n",
        "template <typename T> class RTensor;\n",
        "\n",
        "// Simple tensor class for TMVA SOFIE\n",
        "template <typename T>\n",
        "class RTensor {\n",
        "private:\n",
        "    std::vector<size_t> fShape;\n",
        "    std::shared_ptr<T[]> fData;\n",
        "    size_t fSize;\n",
        "\n",
        "public:\n",
        "    // Default constructor\n",
        "    RTensor() : fSize(0) {}\n",
        "\n",
        "    // Constructor with shape\n",
        "    RTensor(const std::vector<size_t>& shape) : fShape(shape) {\n",
        "        fSize = std::accumulate(shape.begin(), shape.end(),\n",
        "                              (size_t)1, std::multiplies<size_t>());\n",
        "        fData = std::shared_ptr<T[]>(new T[fSize]());\n",
        "    }\n",
        "\n",
        "    // Constructor with shape and data\n",
        "    RTensor(const std::vector<size_t>& shape, const T* data) : fShape(shape) {\n",
        "        fSize = std::accumulate(shape.begin(), shape.end(),\n",
        "                              (size_t)1, std::multiplies<size_t>());\n",
        "        fData = std::shared_ptr<T[]>(new T[fSize]);\n",
        "        std::copy(data, data + fSize, fData.get());\n",
        "    }\n",
        "\n",
        "    // Get shape\n",
        "    const std::vector<size_t>& GetShape() const {\n",
        "        return fShape;\n",
        "    }\n",
        "\n",
        "    // Get size\n",
        "    size_t GetSize() const {\n",
        "        return fSize;\n",
        "    }\n",
        "\n",
        "    // Get data\n",
        "    T* GetData() {\n",
        "        return fData.get();\n",
        "    }\n",
        "\n",
        "    const T* GetData() const {\n",
        "        return fData.get();\n",
        "    }\n",
        "\n",
        "    // Access element\n",
        "    T& operator[](size_t index) {\n",
        "        if (index >= fSize) {\n",
        "            throw std::out_of_range(\"RTensor index out of range\");\n",
        "        }\n",
        "        return fData[index];\n",
        "    }\n",
        "\n",
        "    const T& operator[](size_t index) const {\n",
        "        if (index >= fSize) {\n",
        "            throw std::out_of_range(\"RTensor index out of range\");\n",
        "        }\n",
        "        return fData[index];\n",
        "    }\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_RTENSOR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "susmbGR9XuSZ",
        "outputId": "0760478c-f3a5-4b0d-e093-7a74e253938d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/RTensor.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/SOFIE_common.hxx\n",
        "#ifndef TMVA_SOFIE_COMMON\n",
        "#define TMVA_SOFIE_COMMON\n",
        "\n",
        "#include \"TMVA/RTensor.hxx\"\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "#include <stdexcept>\n",
        "#include <iostream>\n",
        "#include <unordered_map>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Basic tensor type enum\n",
        "enum class ETensorType { FLOAT, DOUBLE, INT64, BOOL };\n",
        "\n",
        "// Dimension structure for dynamic shapes\n",
        "struct Dim {\n",
        "    std::string name;\n",
        "    size_t size;\n",
        "};\n",
        "\n",
        "// Helper functions for shape conversion\n",
        "inline size_t ConvertShapeToLength(const std::vector<size_t>& shape) {\n",
        "    size_t length = 1;\n",
        "    for (auto& dim : shape) {\n",
        "        length *= dim;\n",
        "    }\n",
        "    return length;\n",
        "}\n",
        "\n",
        "// Get string representation of type\n",
        "template<typename T>\n",
        "std::string GetTensorTypeName() {\n",
        "    if (std::is_same<T, float>::value) return \"float\";\n",
        "    if (std::is_same<T, double>::value) return \"double\";\n",
        "    if (std::is_same<T, int64_t>::value) return \"int64_t\";\n",
        "    if (std::is_same<T, bool>::value) return \"bool\";\n",
        "    return \"unknown\";\n",
        "}\n",
        "\n",
        "// Get ETensorType from C++ type\n",
        "template<typename T>\n",
        "ETensorType GetTemplatedType(T) {\n",
        "    if (std::is_same<T, float>::value) return ETensorType::FLOAT;\n",
        "    if (std::is_same<T, double>::value) return ETensorType::DOUBLE;\n",
        "    if (std::is_same<T, int64_t>::value) return ETensorType::INT64;\n",
        "    if (std::is_same<T, bool>::value) return ETensorType::BOOL;\n",
        "    throw std::runtime_error(\"Unsupported type in GetTemplatedType\");\n",
        "}\n",
        "\n",
        "// Simple tensor info structure\n",
        "struct TensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<size_t> shape;\n",
        "};\n",
        "\n",
        "// Structure for dynamic tensor info\n",
        "struct DynamicTensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<Dim> shape;\n",
        "};\n",
        "\n",
        "// Structure for input tensor info\n",
        "struct InputTensorInfo {\n",
        "    ETensorType type;\n",
        "    std::vector<Dim> shape;\n",
        "};\n",
        "\n",
        "// Structure for initialized tensor\n",
        "struct InitializedTensor {\n",
        "    ETensorType type;\n",
        "    std::vector<size_t> shape;\n",
        "    std::shared_ptr<void> data;\n",
        "    bool isConstant = false;\n",
        "    bool isWritable = true;\n",
        "};\n",
        "\n",
        "// Options for code generation\n",
        "enum class Options {\n",
        "    kDefault = 0,\n",
        "    kNoSession = 1\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_COMMON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3uXiVOVXwWa",
        "outputId": "59c58ab4-43a0-4c92-8a2d-742b2f8508a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/SOFIE_common.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/ROperator.hxx\n",
        "#ifndef TMVA_SOFIE_ROPERATOR\n",
        "#define TMVA_SOFIE_ROPERATOR\n",
        "\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <string>\n",
        "#include <vector>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Forward declaration\n",
        "class RModel;\n",
        "\n",
        "// Base class for all operators\n",
        "class ROperator {\n",
        "public:\n",
        "    virtual ~ROperator() = default;\n",
        "    virtual void Initialize(RModel& model) = 0;\n",
        "    virtual std::string Generate(std::string OpName) = 0;\n",
        "\n",
        "    // Common members\n",
        "    std::vector<std::string> fInputTensorNames;\n",
        "    std::vector<std::string> fOutputTensorNames;\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_ROPERATOR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7dkInA3Xze5",
        "outputId": "0093e23b-6c69-4ab3-b19a-724c5427285b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/ROperator.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/RModel.hxx\n",
        "#ifndef TMVA_SOFIE_RMODEL\n",
        "#define TMVA_SOFIE_RMODEL\n",
        "\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <unordered_map>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <memory>\n",
        "#include <map>\n",
        "#include <algorithm>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "// Mock RModel class for our implementation\n",
        "class RModel {\n",
        "private:\n",
        "    std::string fName;\n",
        "    std::string fParsedDateTime;\n",
        "    bool fIsInitialized = false;\n",
        "    int fVerbose = 1;\n",
        "\n",
        "    std::unordered_map<std::string, InputTensorInfo> fInputTensorInfos;\n",
        "    std::unordered_map<std::string, TensorInfo> fReadyInputTensorInfos;\n",
        "    std::unordered_map<std::string, InitializedTensor> fInitializedTensors;\n",
        "    std::unordered_map<std::string, TensorInfo> fIntermediateTensorInfos;\n",
        "    std::unordered_map<std::string, DynamicTensorInfo> fDynamicTensorInfos;\n",
        "    std::vector<std::string> fOutputTensorNames;\n",
        "    std::vector<std::string> fInputTensorNames;\n",
        "\n",
        "public:\n",
        "    RModel() = default;\n",
        "    RModel(std::string name, std::string parsedtime) : fName(name), fParsedDateTime(parsedtime) {}\n",
        "\n",
        "    int Verbose() const { return fVerbose; }\n",
        "\n",
        "    const std::vector<size_t>& GetTensorShape(const std::string& name) {\n",
        "        // First check intermediate tensors\n",
        "        auto it = fIntermediateTensorInfos.find(name);\n",
        "        if (it != fIntermediateTensorInfos.end()) {\n",
        "            return it->second.shape;\n",
        "        }\n",
        "\n",
        "        // Check initialized tensors\n",
        "        auto it2 = fInitializedTensors.find(name);\n",
        "        if (it2 != fInitializedTensors.end()) {\n",
        "            return it2->second.shape;\n",
        "        }\n",
        "\n",
        "        // Check input tensors\n",
        "        auto it3 = fReadyInputTensorInfos.find(name);\n",
        "        if (it3 != fReadyInputTensorInfos.end()) {\n",
        "            return it3->second.shape;\n",
        "        }\n",
        "\n",
        "        throw std::runtime_error(\"Tensor not found: \" + name);\n",
        "    }\n",
        "\n",
        "    const ETensorType& GetTensorType(const std::string& name) {\n",
        "        // First check intermediate tensors\n",
        "        auto it = fIntermediateTensorInfos.find(name);\n",
        "        if (it != fIntermediateTensorInfos.end()) {\n",
        "            return it->second.type;\n",
        "        }\n",
        "\n",
        "        // Check initialized tensors\n",
        "        auto it2 = fInitializedTensors.find(name);\n",
        "        if (it2 != fInitializedTensors.end()) {\n",
        "            return it2->second.type;\n",
        "        }\n",
        "\n",
        "        // Check input tensors\n",
        "        auto it3 = fReadyInputTensorInfos.find(name);\n",
        "        if (it3 != fReadyInputTensorInfos.end()) {\n",
        "            return it3->second.type;\n",
        "        }\n",
        "\n",
        "        throw std::runtime_error(\"Tensor type not found: \" + name);\n",
        "    }\n",
        "\n",
        "    bool CheckIfTensorAlreadyExist(const std::string& name) {\n",
        "        return (fIntermediateTensorInfos.find(name) != fIntermediateTensorInfos.end()) ||\n",
        "               (fInitializedTensors.find(name) != fInitializedTensors.end()) ||\n",
        "               (fReadyInputTensorInfos.find(name) != fReadyInputTensorInfos.end()) ||\n",
        "               (fInputTensorInfos.find(name) != fInputTensorInfos.end());\n",
        "    }\n",
        "\n",
        "    // Add input tensor info with full shape\n",
        "    void AddInputTensorInfo(const std::string& name, ETensorType type, const std::vector<size_t>& shape) {\n",
        "        TensorInfo info;\n",
        "        info.type = type;\n",
        "        info.shape = shape;\n",
        "        fReadyInputTensorInfos[name] = info;\n",
        "\n",
        "        // Also add to input tensor names if not already there\n",
        "        if (std::find(fInputTensorNames.begin(), fInputTensorNames.end(), name) == fInputTensorNames.end()) {\n",
        "            fInputTensorNames.push_back(name);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add intermediate tensor\n",
        "    void AddIntermediateTensor(const std::string& name, ETensorType type, const std::vector<size_t>& shape) {\n",
        "        TensorInfo info;\n",
        "        info.type = type;\n",
        "        info.shape = shape;\n",
        "        fIntermediateTensorInfos[name] = info;\n",
        "    }\n",
        "\n",
        "    // Add output tensor names\n",
        "    void AddOutputTensorNameList(const std::vector<std::string>& names) {\n",
        "        fOutputTensorNames = names;\n",
        "    }\n",
        "\n",
        "    // Initialize model (simplified for mock)\n",
        "    void Initialize(int batchSize = -1) {\n",
        "        fIsInitialized = true;\n",
        "\n",
        "        if (Verbose()) {\n",
        "            std::cout << \"Model initialized with batch size: \" <<\n",
        "                (batchSize == -1 ? \"default\" : std::to_string(batchSize)) << std::endl;\n",
        "        }\n",
        "    }\n",
        "};\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_RMODEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWV_FNHVX3xh",
        "outputId": "6bcf2654-18fe-4f78-de7c-4b05dbe1045a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/RModel.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/ROperator_Relu_CUDA.hxx\n",
        "#ifndef TMVA_SOFIE_ROPERATOR_RELU_CUDA\n",
        "#define TMVA_SOFIE_ROPERATOR_RELU_CUDA\n",
        "\n",
        "#include \"TMVA/ROperator.hxx\"\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "class ROperator_Relu_CUDA final : public ROperator\n",
        "{\n",
        "private:\n",
        "   std::string fNX;      // Input tensor name\n",
        "   std::string fNY;      // Output tensor name\n",
        "   std::vector<size_t> fShape;  // Tensor shape\n",
        "\n",
        "public:\n",
        "   ROperator_Relu_CUDA() = default;\n",
        "\n",
        "   ROperator_Relu_CUDA(std::string nameX, std::string nameY):\n",
        "      fNX(nameX), fNY(nameY) {\n",
        "         fInputTensorNames = { nameX };\n",
        "         fOutputTensorNames = { nameY };\n",
        "      }\n",
        "\n",
        "   // Type and shape inference\n",
        "   std::vector<ETensorType> TypeInference(std::vector<ETensorType> input) {\n",
        "      return input;  // ReLU preserves input type\n",
        "   }\n",
        "\n",
        "   std::vector<std::vector<size_t>> ShapeInference(std::vector<std::vector<size_t>> input) {\n",
        "      return input;  // ReLU preserves input shape\n",
        "   }\n",
        "\n",
        "   // Required ROperator interface methods\n",
        "   void Initialize(RModel& model) override;\n",
        "   std::string Generate(std::string OpName) override;\n",
        "};\n",
        "\n",
        "// Declare template specializations\n",
        "extern template class ROperator_Relu_CUDA<float>;\n",
        "extern template class ROperator_Relu_CUDA<double>;\n",
        "extern template class ROperator_Relu_CUDA<int64_t>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_ROPERATOR_RELU_CUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYN2VHAgX69W",
        "outputId": "fe9c8021-bbe1-4c2c-b8f0-549064511e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/include/TMVA/ROperator_Relu_CUDA.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu\n",
        "#include \"TMVA/ROperator_Relu_CUDA.hxx\"\n",
        "#include <sstream>\n",
        "\n",
        "// CUDA kernel for ReLU operation\n",
        "__global__ void reluKernelFloat(const float* input, float* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = fmaxf(0.0f, input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for ReLU with double precision\n",
        "__global__ void reluKernelDouble(const double* input, double* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = fmax(0.0, input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for ReLU with int64\n",
        "__global__ void reluKernelInt64(const int64_t* input, int64_t* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = (input[idx] > 0) ? input[idx] : 0;\n",
        "    }\n",
        "}\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "void ROperator_Relu_CUDA<T>::Initialize(RModel& model)\n",
        "{\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Relu CUDA: Input tensor \" + fNX + \" not found in model\");\n",
        "    }\n",
        "\n",
        "    // Get shape from the model\n",
        "    fShape = model.GetTensorShape(fNX);\n",
        "\n",
        "    // Add output tensor to the model with same type and shape as input\n",
        "    model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
        "\n",
        "    if (model.Verbose()) {\n",
        "        std::cout << \"TMVA SOFIE Relu CUDA: \" << fNX << \" -> \" << fNY << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "std::string ROperator_Relu_CUDA<T>::Generate(std::string OpName)\n",
        "{\n",
        "    if (fShape.empty()) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Relu CUDA: Called Generate without initialization\");\n",
        "    }\n",
        "\n",
        "    std::stringstream out;\n",
        "    size_t length = 1;\n",
        "    for (auto& dim : fShape) {\n",
        "        length *= dim;\n",
        "    }\n",
        "\n",
        "    std::string typeName = GetTensorTypeName<T>();\n",
        "\n",
        "    // Begin code generation\n",
        "    out << \"\\n// \" << OpName << \" ReLU CUDA implementation\\n\";\n",
        "\n",
        "    // 1. Define the kernel\n",
        "    out << \"__global__ void \" << OpName << \"_relu_kernel(const \" << typeName << \"* input, \"\n",
        "        << typeName << \"* output, size_t size) {\\n\";\n",
        "    out << \"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\\n\";\n",
        "    out << \"    if (idx < size) {\\n\";\n",
        "\n",
        "    // Type-specific implementation\n",
        "    if (std::is_same<T, float>::value) {\n",
        "        out << \"        output[idx] = fmaxf(0.0f, input[idx]);\\n\";\n",
        "    } else if (std::is_same<T, double>::value) {\n",
        "        out << \"        output[idx] = fmax(0.0, input[idx]);\\n\";\n",
        "    } else {\n",
        "        out << \"        output[idx] = (input[idx] > 0) ? input[idx] : 0;\\n\";\n",
        "    }\n",
        "\n",
        "    out << \"    }\\n\";\n",
        "    out << \"}\\n\\n\";\n",
        "\n",
        "    // 2. Execution code block\n",
        "    out << \"{\\n\";  // Begin scope\n",
        "\n",
        "    // Calculate launch configuration\n",
        "    out << \"    // Calculate execution configuration\\n\";\n",
        "    out << \"    size_t size = \" << length << \";\\n\";\n",
        "    out << \"    int blockSize = 256;\\n\";\n",
        "    out << \"    int numBlocks = (size + blockSize - 1) / blockSize;\\n\\n\";\n",
        "\n",
        "    // GPU Memory allocation\n",
        "    out << \"    // Allocate device memory\\n\";\n",
        "    out << \"    \" << typeName << \"* d_input = nullptr;\\n\";\n",
        "    out << \"    \" << typeName << \"* d_output = nullptr;\\n\";\n",
        "    out << \"    cudaError_t cudaStatus;\\n\\n\";\n",
        "\n",
        "    // Error handling and memory management\n",
        "    out << \"    // CUDA memory allocation\\n\";\n",
        "    out << \"    cudaStatus = cudaMalloc(&d_input, size * sizeof(\" << typeName << \"));\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMalloc failed for input: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    out << \"    cudaStatus = cudaMalloc(&d_output, size * sizeof(\" << typeName << \"));\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMalloc failed for output: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Copy input to device\n",
        "    out << \"    // Copy input to device\\n\";\n",
        "    out << \"    cudaStatus = cudaMemcpy(d_input, tensor_\" << fNX << \", size * sizeof(\" << typeName << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMemcpy to device failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Launch kernel\n",
        "    out << \"    // Launch kernel\\n\";\n",
        "    out << \"    \" << OpName << \"_relu_kernel<<<numBlocks, blockSize>>>(d_input, d_output, size);\\n\\n\";\n",
        "\n",
        "    // Check for kernel errors\n",
        "    out << \"    // Check for kernel errors\\n\";\n",
        "    out << \"    cudaStatus = cudaGetLastError();\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"CUDA kernel launch failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Synchronize\n",
        "    out << \"    // Wait for kernel completion\\n\";\n",
        "    out << \"    cudaStatus = cudaDeviceSynchronize();\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaDeviceSynchronize failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Copy result back to host\n",
        "    out << \"    // Copy result back to host\\n\";\n",
        "    out << \"    cudaStatus = cudaMemcpy(tensor_\" << fNY << \", d_output, size * sizeof(\" << typeName << \"), cudaMemcpyDeviceToHost);\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMemcpy to host failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Cleanup section\n",
        "    out << OpName << \"_cleanup:\\n\";\n",
        "    out << \"    // Clean up device memory\\n\";\n",
        "    out << \"    if (d_input) cudaFree(d_input);\\n\";\n",
        "    out << \"    if (d_output) cudaFree(d_output);\\n\\n\";\n",
        "\n",
        "    // CPU fallback if CUDA fails\n",
        "    out << \"    // CPU fallback if CUDA execution failed\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"Using CPU fallback for ReLU operation\\\" << std::endl;\\n\";\n",
        "    out << \"        for (size_t i = 0; i < size; i++) {\\n\";\n",
        "    out << \"            tensor_\" << fNY << \"[i] = (tensor_\" << fNX << \"[i] > 0) ? tensor_\" << fNX << \"[i] : 0;\\n\";\n",
        "    out << \"        }\\n\";\n",
        "    out << \"    }\\n\";\n",
        "\n",
        "    out << \"}\\n\";  // End scope\n",
        "\n",
        "    return out.str();\n",
        "}\n",
        "\n",
        "// Explicit template instantiations\n",
        "template class ROperator_Relu_CUDA<float>;\n",
        "template class ROperator_Relu_CUDA<double>;\n",
        "template class ROperator_Relu_CUDA<int64_t>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHMPoh9aX_t1",
        "outputId": "3efb8beb-f102-4720-d980-306443450be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/test/test_relu_cuda.cu\n",
        "#include \"TMVA/ROperator_Relu_CUDA.hxx\"\n",
        "#include \"TMVA/RModel.hxx\"\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <iomanip>\n",
        "\n",
        "using namespace TMVA::Experimental::SOFIE;\n",
        "\n",
        "// Function to print tensor data\n",
        "template <typename T>\n",
        "void printTensor(const std::vector<T>& data, const std::vector<size_t>& shape) {\n",
        "    if (shape.size() == 1) {\n",
        "        for (size_t i = 0; i < std::min(data.size(), size_t(10)); i++) {\n",
        "            std::cout << std::fixed << std::setprecision(2) << data[i] << \" \";\n",
        "        }\n",
        "        if (data.size() > 10) std::cout << \"...\";\n",
        "        std::cout << std::endl;\n",
        "    } else if (shape.size() == 2) {\n",
        "        for (size_t i = 0; i < std::min(shape[0], size_t(5)); i++) {\n",
        "            for (size_t j = 0; j < std::min(shape[1], size_t(10)); j++) {\n",
        "                std::cout << std::fixed << std::setprecision(2) << data[i * shape[1] + j] << \" \";\n",
        "            }\n",
        "            if (shape[1] > 10) std::cout << \"...\";\n",
        "            std::cout << std::endl;\n",
        "        }\n",
        "        if (shape[0] > 5) std::cout << \"...\" << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    std::cout << \"Testing TMVA SOFIE CUDA ReLU Operator\" << std::endl;\n",
        "    std::cout << \"=====================================\" << std::endl;\n",
        "\n",
        "    try {\n",
        "        // Create a model\n",
        "        RModel model(\"cuda_relu_test\", \"2025-03-14\");\n",
        "\n",
        "        // Create input tensor shape and add to model\n",
        "        std::vector<size_t> shape = {4, 4};\n",
        "        model.AddInputTensorInfo(\"input\", ETensorType::FLOAT, shape);\n",
        "\n",
        "        // Initialize the model\n",
        "        model.Initialize();\n",
        "\n",
        "        // Create ReLU CUDA operator\n",
        "        ROperator_Relu_CUDA<float> reluOp(\"input\", \"output\");\n",
        "\n",
        "        // Initialize operator\n",
        "        reluOp.Initialize(model);\n",
        "\n",
        "        // Generate code\n",
        "        std::string generatedCode = reluOp.Generate(\"TestRelu\");\n",
        "\n",
        "        // Print code excerpt\n",
        "        std::cout << \"\\nGenerated CUDA code (excerpt):\" << std::endl;\n",
        "        std::cout << \"----------------------------\" << std::endl;\n",
        "        std::cout << generatedCode.substr(0, 300) << \"...\\n\" << std::endl;\n",
        "\n",
        "        std::cout << \"\\nReLU CUDA operator test completed successfully!\" << std::endl;\n",
        "\n",
        "        return 0;\n",
        "    } catch (const std::exception& e) {\n",
        "        std::cerr << \"Error: \" << e.what() << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wTwMOu8YGtT",
        "outputId": "afcd9908-fcd9-4e74-f571-9d3c1303de21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/test/test_relu_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.10)\n",
        "project(TMVA_SOFIE_CUDA CUDA CXX)\n",
        "\n",
        "# Set C++ standard\n",
        "set(CMAKE_CXX_STANDARD 14)\n",
        "set(CMAKE_CUDA_STANDARD 14)\n",
        "set(CMAKE_CUDA_ARCHITECTURES 70)\n",
        "\n",
        "# Find CUDA\n",
        "find_package(CUDA REQUIRED)\n",
        "\n",
        "# Include directories\n",
        "include_directories(\n",
        "    ${CMAKE_CURRENT_SOURCE_DIR}/include\n",
        "    ${CUDA_INCLUDE_DIRS}\n",
        ")\n",
        "\n",
        "# Add CUDA operator implementation\n",
        "cuda_add_executable(test_relu_cuda\n",
        "    test/test_relu_cuda.cu\n",
        "    src/ROperator_Relu_CUDA.cu\n",
        ")\n",
        "\n",
        "# Link against CUDA libraries\n",
        "target_link_libraries(test_relu_cuda\n",
        "    ${CUDA_LIBRARIES}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf2612DdYKfC",
        "outputId": "b4739539-df7b-4de5-99b9-1d63c6454ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tmva_cuda_project/CMakeLists.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the project\n",
        "!cd /content/tmva_cuda_project && cmake -B build && cmake --build build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gov8Nh-nYMy1",
        "outputId": "b0e3a514-2ec2-4e71-b70a-3713963c25cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning (dev) at CMakeLists.txt:10 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "-- Configuring done (4.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/tmva_cuda_project/build\n",
            "[ 33%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_relu_cuda.dir/test/test_relu_cuda_generated_test_relu_cuda.cu.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_relu_cuda.dir/src/test_relu_cuda_generated_ROperator_Relu_CUDA.cu.o\u001b[0m\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(35): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
            "           ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=float]\" at line 172\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(40): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      fShape = model.GetTensorShape(fNX);\n",
            "               ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=float]\" at line 172\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "      ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=float]\" at line 172\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "                                       ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=float]\" at line 172\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(45): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (model.Verbose()) {\n",
            "          ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=float]\" at line 172\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(35): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
            "           ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=double]\" at line 173\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(40): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      fShape = model.GetTensorShape(fNX);\n",
            "               ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=double]\" at line 173\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "      ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=double]\" at line 173\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "                                       ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=double]\" at line 173\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(45): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (model.Verbose()) {\n",
            "          ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=double]\" at line 173\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(35): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
            "           ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=int64_t]\" at line 174\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(40): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      fShape = model.GetTensorShape(fNX);\n",
            "               ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=int64_t]\" at line 174\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "      ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=int64_t]\" at line 174\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(43): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
            "                                       ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=int64_t]\" at line 174\n",
            "\n",
            "/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu(45): error: incomplete type \"TMVA::Experimental::SOFIE::RModel\" is not allowed\n",
            "      if (model.Verbose()) {\n",
            "          ^\n",
            "          detected during instantiation of \"void TMVA::Experimental::SOFIE::ROperator_Relu_CUDA<T>::Initialize(TMVA::Experimental::SOFIE::RModel &) [with T=int64_t]\" at line 174\n",
            "\n",
            "15 errors detected in the compilation of \"/content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu\".\n",
            "\u001b[31mCMake Error at test_relu_cuda_generated_ROperator_Relu_CUDA.cu.o.cmake:280 (message):\n",
            "  Error generating file\n",
            "  /content/tmva_cuda_project/build/CMakeFiles/test_relu_cuda.dir/src/./test_relu_cuda_generated_ROperator_Relu_CUDA.cu.o\n",
            "\n",
            "\u001b[0m\n",
            "gmake[2]: *** [CMakeFiles/test_relu_cuda.dir/build.make:84: CMakeFiles/test_relu_cuda.dir/src/test_relu_cuda_generated_ROperator_Relu_CUDA.cu.o] Error 1\n",
            "gmake[1]: *** [CMakeFiles/Makefile2:86: CMakeFiles/test_relu_cuda.dir/all] Error 2\n",
            "gmake: *** [Makefile:91: all] Error 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's update the ROperator_Relu_CUDA.cu file to include RModel.hxx explicitly\n",
        "%%writefile /content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu\n",
        "#include \"TMVA/ROperator_Relu_CUDA.hxx\"\n",
        "#include \"TMVA/RModel.hxx\"  // Explicitly include RModel.hxx\n",
        "#include <sstream>\n",
        "\n",
        "// CUDA kernel for ReLU operation\n",
        "__global__ void reluKernelFloat(const float* input, float* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = fmaxf(0.0f, input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for ReLU with double precision\n",
        "__global__ void reluKernelDouble(const double* input, double* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = fmax(0.0, input[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for ReLU with int64\n",
        "__global__ void reluKernelInt64(const int64_t* input, int64_t* output, size_t size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < size) {\n",
        "        output[idx] = (input[idx] > 0) ? input[idx] : 0;\n",
        "    }\n",
        "}\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "void ROperator_Relu_CUDA<T>::Initialize(RModel& model)\n",
        "{\n",
        "    if (!model.CheckIfTensorAlreadyExist(fNX)) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Relu CUDA: Input tensor \" + fNX + \" not found in model\");\n",
        "    }\n",
        "\n",
        "    // Get shape from the model\n",
        "    fShape = model.GetTensorShape(fNX);\n",
        "\n",
        "    // Add output tensor to the model with same type and shape as input\n",
        "    model.AddIntermediateTensor(fNY, model.GetTensorType(fNX), fShape);\n",
        "\n",
        "    if (model.Verbose()) {\n",
        "        std::cout << \"TMVA SOFIE Relu CUDA: \" << fNX << \" -> \" << fNY << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "std::string ROperator_Relu_CUDA<T>::Generate(std::string OpName)\n",
        "{\n",
        "    if (fShape.empty()) {\n",
        "        throw std::runtime_error(\"TMVA SOFIE Relu CUDA: Called Generate without initialization\");\n",
        "    }\n",
        "\n",
        "    std::stringstream out;\n",
        "    size_t length = 1;\n",
        "    for (auto& dim : fShape) {\n",
        "        length *= dim;\n",
        "    }\n",
        "\n",
        "    std::string typeName = GetTensorTypeName<T>();\n",
        "\n",
        "    // Begin code generation\n",
        "    out << \"\\n// \" << OpName << \" ReLU CUDA implementation\\n\";\n",
        "\n",
        "    // 1. Define the kernel\n",
        "    out << \"__global__ void \" << OpName << \"_relu_kernel(const \" << typeName << \"* input, \"\n",
        "        << typeName << \"* output, size_t size) {\\n\";\n",
        "    out << \"    int idx = blockIdx.x * blockDim.x + threadIdx.x;\\n\";\n",
        "    out << \"    if (idx < size) {\\n\";\n",
        "\n",
        "    // Type-specific implementation\n",
        "    if (std::is_same<T, float>::value) {\n",
        "        out << \"        output[idx] = fmaxf(0.0f, input[idx]);\\n\";\n",
        "    } else if (std::is_same<T, double>::value) {\n",
        "        out << \"        output[idx] = fmax(0.0, input[idx]);\\n\";\n",
        "    } else {\n",
        "        out << \"        output[idx] = (input[idx] > 0) ? input[idx] : 0;\\n\";\n",
        "    }\n",
        "\n",
        "    out << \"    }\\n\";\n",
        "    out << \"}\\n\\n\";\n",
        "\n",
        "    // 2. Execution code block\n",
        "    out << \"{\\n\";  // Begin scope\n",
        "\n",
        "    // Calculate launch configuration\n",
        "    out << \"    // Calculate execution configuration\\n\";\n",
        "    out << \"    size_t size = \" << length << \";\\n\";\n",
        "    out << \"    int blockSize = 256;\\n\";\n",
        "    out << \"    int numBlocks = (size + blockSize - 1) / blockSize;\\n\\n\";\n",
        "\n",
        "    // GPU Memory allocation\n",
        "    out << \"    // Allocate device memory\\n\";\n",
        "    out << \"    \" << typeName << \"* d_input = nullptr;\\n\";\n",
        "    out << \"    \" << typeName << \"* d_output = nullptr;\\n\";\n",
        "    out << \"    cudaError_t cudaStatus;\\n\\n\";\n",
        "\n",
        "    // Error handling and memory management\n",
        "    out << \"    // CUDA memory allocation\\n\";\n",
        "    out << \"    cudaStatus = cudaMalloc(&d_input, size * sizeof(\" << typeName << \"));\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMalloc failed for input: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    out << \"    cudaStatus = cudaMalloc(&d_output, size * sizeof(\" << typeName << \"));\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMalloc failed for output: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Copy input to device\n",
        "    out << \"    // Copy input to device\\n\";\n",
        "    out << \"    cudaStatus = cudaMemcpy(d_input, tensor_\" << fNX << \", size * sizeof(\" << typeName << \"), cudaMemcpyHostToDevice);\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMemcpy to device failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Launch kernel\n",
        "    out << \"    // Launch kernel\\n\";\n",
        "    out << \"    \" << OpName << \"_relu_kernel<<<numBlocks, blockSize>>>(d_input, d_output, size);\\n\\n\";\n",
        "\n",
        "    // Check for kernel errors\n",
        "    out << \"    // Check for kernel errors\\n\";\n",
        "    out << \"    cudaStatus = cudaGetLastError();\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"CUDA kernel launch failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Synchronize\n",
        "    out << \"    // Wait for kernel completion\\n\";\n",
        "    out << \"    cudaStatus = cudaDeviceSynchronize();\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaDeviceSynchronize failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Copy result back to host\n",
        "    out << \"    // Copy result back to host\\n\";\n",
        "    out << \"    cudaStatus = cudaMemcpy(tensor_\" << fNY << \", d_output, size * sizeof(\" << typeName << \"), cudaMemcpyDeviceToHost);\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"cudaMemcpy to host failed: \\\" << cudaGetErrorString(cudaStatus) << std::endl;\\n\";\n",
        "    out << \"        goto \" << OpName << \"_cleanup;\\n\";\n",
        "    out << \"    }\\n\\n\";\n",
        "\n",
        "    // Cleanup section\n",
        "    out << OpName << \"_cleanup:\\n\";\n",
        "    out << \"    // Clean up device memory\\n\";\n",
        "    out << \"    if (d_input) cudaFree(d_input);\\n\";\n",
        "    out << \"    if (d_output) cudaFree(d_output);\\n\\n\";\n",
        "\n",
        "    // CPU fallback if CUDA fails\n",
        "    out << \"    // CPU fallback if CUDA execution failed\\n\";\n",
        "    out << \"    if (cudaStatus != cudaSuccess) {\\n\";\n",
        "    out << \"        std::cerr << \\\"Using CPU fallback for ReLU operation\\\" << std::endl;\\n\";\n",
        "    out << \"        for (size_t i = 0; i < size; i++) {\\n\";\n",
        "    out << \"            tensor_\" << fNY << \"[i] = (tensor_\" << fNX << \"[i] > 0) ? tensor_\" << fNX << \"[i] : 0;\\n\";\n",
        "    out << \"        }\\n\";\n",
        "    out << \"    }\\n\";\n",
        "\n",
        "    out << \"}\\n\";  // End scope\n",
        "\n",
        "    return out.str();\n",
        "}\n",
        "\n",
        "// Explicit template instantiations\n",
        "template class ROperator_Relu_CUDA<float>;\n",
        "template class ROperator_Relu_CUDA<double>;\n",
        "template class ROperator_Relu_CUDA<int64_t>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8kSxu3cYOya",
        "outputId": "29588d03-bf1a-4f1e-f075-bb734b9c1a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tmva_cuda_project/src/ROperator_Relu_CUDA.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/tmva_cuda_project/include/TMVA/ROperator_Relu_CUDA.hxx\n",
        "#ifndef TMVA_SOFIE_ROPERATOR_RELU_CUDA\n",
        "#define TMVA_SOFIE_ROPERATOR_RELU_CUDA\n",
        "\n",
        "#include \"TMVA/RModel.hxx\"  // Include RModel.hxx first\n",
        "#include \"TMVA/ROperator.hxx\"\n",
        "#include \"TMVA/SOFIE_common.hxx\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "\n",
        "namespace TMVA {\n",
        "namespace Experimental {\n",
        "namespace SOFIE {\n",
        "\n",
        "template <typename T>\n",
        "class ROperator_Relu_CUDA final : public ROperator\n",
        "{\n",
        "private:\n",
        "   std::string fNX;      // Input tensor name\n",
        "   std::string fNY;      // Output tensor name\n",
        "   std::vector<size_t> fShape;  // Tensor shape\n",
        "\n",
        "public:\n",
        "   ROperator_Relu_CUDA() = default;\n",
        "\n",
        "   ROperator_Relu_CUDA(std::string nameX, std::string nameY):\n",
        "      fNX(nameX), fNY(nameY) {\n",
        "         fInputTensorNames = { nameX };\n",
        "         fOutputTensorNames = { nameY };\n",
        "      }\n",
        "\n",
        "   // Type and shape inference\n",
        "   std::vector<ETensorType> TypeInference(std::vector<ETensorType> input) {\n",
        "      return input;  // ReLU preserves input type\n",
        "   }\n",
        "\n",
        "   std::vector<std::vector<size_t>> ShapeInference(std::vector<std::vector<size_t>> input) {\n",
        "      return input;  // ReLU preserves input shape\n",
        "   }\n",
        "\n",
        "   // Required ROperator interface methods\n",
        "   void Initialize(RModel& model) override;\n",
        "   std::string Generate(std::string OpName) override;\n",
        "};\n",
        "\n",
        "// Declare template specializations\n",
        "extern template class ROperator_Relu_CUDA<float>;\n",
        "extern template class ROperator_Relu_CUDA<double>;\n",
        "extern template class ROperator_Relu_CUDA<int64_t>;\n",
        "\n",
        "}}} // namespace TMVA::Experimental::SOFIE\n",
        "\n",
        "#endif // TMVA_SOFIE_ROPERATOR_RELU_CUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZucZvzv0Yq2E",
        "outputId": "be1b8e81-d764-4633-9641-e167db2f6dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/tmva_cuda_project/include/TMVA/ROperator_Relu_CUDA.hxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and rebuild the project\n",
        "!cd /content/tmva_cuda_project && rm -rf build && mkdir -p build && cd build && cmake .. && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gztt-AXYuRH",
        "outputId": "aa032991-140c-4581-be02-ba3408a43d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[33mCMake Warning (dev) at CMakeLists.txt:10 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "-- Configuring done (2.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/tmva_cuda_project/build\n",
            "[ 33%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_relu_cuda.dir/test/test_relu_cuda_generated_test_relu_cuda.cu.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/test_relu_cuda.dir/src/test_relu_cuda_generated_ROperator_Relu_CUDA.cu.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable test_relu_cuda\u001b[0m\n",
            "[100%] Built target test_relu_cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test\n",
        "!cd /content/tmva_cuda_project/build && ./test_relu_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJx8_IajYw7N",
        "outputId": "85552897-2486-46e6-a0c3-47a41ef4ad1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing TMVA SOFIE CUDA ReLU Operator\n",
            "=====================================\n",
            "Model initialized with batch size: default\n",
            "TMVA SOFIE Relu CUDA: input -> output\n",
            "\n",
            "Generated CUDA code (excerpt):\n",
            "----------------------------\n",
            "\n",
            "// TestRelu ReLU CUDA implementation\n",
            "__global__ void TestRelu_relu_kernel(const float* input, float* output, size_t size) {\n",
            "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    if (idx < size) {\n",
            "        output[idx] = fmaxf(0.0f, input[idx]);\n",
            "    }\n",
            "}\n",
            "\n",
            "{\n",
            "    // Calculate execution configuration\n",
            " ...\n",
            "\n",
            "\n",
            "ReLU CUDA operator test completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bn_DZVCQY5Tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}